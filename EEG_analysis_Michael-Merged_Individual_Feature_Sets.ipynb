{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from pandas import *\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438832246.06 Merging...\n",
      "1438832246.06 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_data.csv\n",
      "1438832246.91 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_data.csv\n",
      "Merge Complete: 2.03699994087 total seconds\n",
      "1438832248.11 Merging...\n",
      "1438832248.11 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_events.csv\n",
      "1438832248.24 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_events.csv\n",
      "Merge Complete: 0.526999950409 total seconds\n",
      "1438832248.64 Merging...\n",
      "1438832248.64 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_data.csv\n",
      "Merge Complete: 1.30500006676 total seconds\n",
      "1438832249.94 Merging...\n",
      "1438832249.94 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_events.csv\n",
      "Merge Complete: 0.248999834061 total seconds\n"
     ]
    }
   ],
   "source": [
    "#Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "    df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_data_filenames = glob.glob(path + \"/subj1_series[1-2]*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Train_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Train_Array_Lengths.append(len(df))\n",
    "train_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_event_filenames = glob.glob(path + \"/subj1_series[1-2]*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "train_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same thing and now get your testing data\n",
    "test_data_filenames = glob.glob(path + \"/subj1_series3*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Test_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Test_Array_Lengths.append(len(df))\n",
    "test_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "test_event_filenames = glob.glob(path + \"/subj1_series3*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "test_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the data into the 6 events and into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Labels_HandStart =  train_events['HandStart'].to_sparse(fill_value=0)\n",
    "Train_Labels_FirstDigitTouch =  train_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothStartLoadPhase =  train_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Train_Labels_LiftOff =  train_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Train_Labels_Replace =  train_events['Replace'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothReleased =  train_events['BothReleased'].to_sparse(fill_value=0)\n",
    "# Train_Labels_Combined = train_events.HandStart.map(str) + ',' + train_events.FirstDigitTouch.map(str) + ',' + train_events.BothStartLoadPhase.map(str) + ',' + train_events.LiftOff.map(str) + ',' + train_events.Replace.map(str) + ',' + train_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "Train_Data = train_data.iloc[0:,1:].astype('float32') # select only column data\n",
    "\n",
    "\n",
    "Test_Labels_HandStart =  test_events['HandStart'].to_sparse(fill_value=0)\n",
    "Test_Labels_FirstDigitTouch =  test_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothStartLoadPhase =  test_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Test_Labels_LiftOff =  test_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Test_Labels_Replace =  test_events['Replace'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothReleased =  test_events['BothReleased'].to_sparse(fill_value=0)\n",
    "\n",
    "\n",
    "Test_Data = test_data.iloc[0:,1:].astype('float32') # select only data columns\n",
    "\n",
    "#delete what isn't needed anymore to save memory. \n",
    "del train_data\n",
    "del test_data\n",
    "del train_events\n",
    "del test_events\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#bin the time \n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "##filters borrowed from Nihar. \n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extractFeatures_PCA(Train_Data,Test_Data, PercentVarExplained):\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    PCA_Projections = pca.transform(Train_Data)[:,0:NumPCs]\n",
    "    PCA_Projections_Test = pca.transform(Test_Data)[:,0:NumPCs]\n",
    "    return np.float32(PCA_Projections),np.float32(PCA_Projections_Test)\n",
    "\n",
    "\n",
    "\n",
    "#return rolling mean of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_mean(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_mean(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'mean' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling variance of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_var(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_var(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'var' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling min of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_min(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_min(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'min' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling max of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_max(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_max(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'max' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_skew(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_skew(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'skew' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_kurt(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_kurt(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'kurt' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'Pct'+ quantile + \"_\" + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_corr(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(pd.rolling_corr(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_cov(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(rolling_cov(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Feature Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes rolling statistics for different windows of time for each column of the dataframe. It then does an RFE feature selection technique to keep only the top features for each rolling stat for each of the six labels. It then appends all of this together into a customized feature array for each of the six labels.  \n",
    "\n",
    "This took about 10 minutes on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculations With Window of 10 Complete:  14  Seconds\n",
      "Calculations With Window of 100 Complete:  26  Seconds\n",
      "Calculations With Window of 500 Complete:  37  Seconds\n",
      "Calculations With Window of 700 Complete:  48  Seconds\n",
      "399 Total Resultant Features\n"
     ]
    }
   ],
   "source": [
    "Windows = [10,100,500,700]\n",
    "\n",
    "Test_Features = []\n",
    "Train_Features = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "for w in Windows:\n",
    "    Raw_Data_Rolling_Window = w\n",
    "    \n",
    "    \n",
    "    RawData_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Var_Train = df_rolling_var(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Train = df_rolling_min(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Train = df_rolling_max(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "\n",
    "    RawData_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Var_Test = df_rolling_var(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Test = df_rolling_min(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Test = df_rolling_max(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "    print \"Calculations With Window of\", w, \"Complete: \", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "    \n",
    "    #Append all the features into 1\n",
    "    Train_Features.append(np.concatenate((RawData_Rolling_Means_Train,RawData_Rolling_Var_Train,RawData_Rolling_Skewness_Train,RawData_Rolling_Min_Train,RawData_Rolling_Max_Train),axis = 1))\n",
    "    Test_Features.append(np.concatenate((RawData_Rolling_Means_Test,RawData_Rolling_Var_Test,RawData_Rolling_Skewness_Test,RawData_Rolling_Min_Test,RawData_Rolling_Max_Test),axis = 1))\n",
    "    \n",
    "\n",
    "\n",
    "# Train_Bin_List = []\n",
    "# for i in range (0,len(Train_Array_Lengths)):\n",
    "#     Train_Bin_List.append(Bin_Time(Train_Array_Lengths[i],Initial_Num_Bins)) # bin the time based on the sizes of each series. \n",
    "# Train_Time_Bins = np.uint8(np.concatenate(Train_Bin_List, axis = 0))\n",
    "\n",
    "\n",
    "# Test_Bin_List = []\n",
    "# for i in range (0,len(Test_Array_Lengths)):\n",
    "#     Test_Bin_List.append(Bin_Time(Test_Array_Lengths[i],Initial_Num_Bins)) # bin the time based on the sizes of each series. \n",
    "# Test_Time_Bins = np.uint8(np.concatenate(Test_Bin_List, axis = 0))\n",
    "\n",
    "# print \"Time Bin Creation Complete: \", int(time.time()-Start_Time), \" Seconds\"    \n",
    "\n",
    "# Train_Features.append(Train_Time_Bins) # Add reduced set of features to full set\n",
    "# Test_Features.append(Test_Time_Bins) # Add reduced set of features to full set    \n",
    "# print \"Time Bins Added to Feature Set: \", int(time.time()-Start_Time), \" Seconds\"    \n",
    "# #delete what isn't needed anymore to save memory. \n",
    "# del Train_Time_Bins\n",
    "# del Test_Time_Bins\n",
    "\n",
    "del RawData_Rolling_Means_Train\n",
    "del RawData_Rolling_Means_Test\n",
    "\n",
    "del RawData_Rolling_Var_Train\n",
    "del RawData_Rolling_Var_Test\n",
    "\n",
    "del RawData_Rolling_Skewness_Train\n",
    "del RawData_Rolling_Skewness_Test\n",
    "\n",
    "del RawData_Rolling_Min_Train\n",
    "del RawData_Rolling_Min_Test\n",
    "\n",
    "del RawData_Rolling_Max_Train\n",
    "del RawData_Rolling_Max_Test\n",
    "gc.collect()\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features,Test_Data),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features,Train_Data),axis = 1))\n",
    "\n",
    "del Train_Data\n",
    "del Test_Data\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print Train_Features.shape[1], \"Total Resultant Features\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add additional features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of our feature set before modeling begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Complete: 33  Seconds\n",
      "\n",
      "82 Resultant Principal Components\n"
     ]
    }
   ],
   "source": [
    "#set the percent of the total variance explained we want explained by our subset of principal components. \n",
    "Pct_Variance_Explained = .93\n",
    "\n",
    "\n",
    "Start_Time = time.time() # begin timer\n",
    "\n",
    "# we must first scale the data. \n",
    "Scale_Center = StandardScaler()\n",
    "Z_Train_Features = np.float16(Scale_Center.fit_transform(np.array(Train_Features)))\n",
    "del Train_Features \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "Z_Test_Features = np.float16(Scale_Center.fit_transform(np.array(Test_Features)))\n",
    "del Test_Features \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "PCs_Train,PCs_Test = extractFeatures_PCA(Z_Train_Features,Z_Test_Features,Pct_Variance_Explained)\n",
    "del Z_Train_Features,Z_Test_Features \n",
    "gc.collect()\n",
    "print \"PCA Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "print ''\n",
    "print PCs_Train.shape[1], \"Resultant Principal Components\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391450L, 58L)\n"
     ]
    }
   ],
   "source": [
    "print PCs_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart_PCs Complete: 15  Seconds\n",
      "FirstDigitTouch_PCs Complete: 30  Seconds\n",
      "BothStartLoadPhase_PCs Complete: 44  Seconds\n",
      "LiftOff_PCs Complete: 60  Seconds\n",
      "Replace_PCs Complete: 75  Seconds\n",
      "BothReleased_PCs Complete: 89  Seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Complete: 1  Seconds\n",
      "HandStart_Test Features Complete: 169  Seconds\n",
      "FirstDigitTouch Features Complete: 237  Seconds\n",
      "BothStartLoadPhase Features Complete: 342  Seconds\n",
      "LiftOff Features Complete: 499  Seconds\n",
      "Replace Features Complete: 539  Seconds\n",
      "BothReleased Features Complete: 585  Seconds\n",
      "68 Features Remaining\n"
     ]
    }
   ],
   "source": [
    "Pct_Features_To_Keep = .90\n",
    "Num_Features = PCs_Train.shape[1]\n",
    "\n",
    "Num_Features_To_Keep = int(Num_Features*Pct_Features_To_Keep)\n",
    "Num_Steps = 4\n",
    "\n",
    "\n",
    "C_Value = 1\n",
    "penalty = 'l1'\n",
    "Convergence_tol = .01\n",
    "\n",
    "\n",
    "RFE_Step = int((Num_Features-Num_Features_To_Keep)/Num_Steps)\n",
    "\n",
    "Start_Time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "#since the recursive feature elimination uses the coefficients as a means to eliminate features, \n",
    "# we must first scale the data. \n",
    "\n",
    "Scale_Center = StandardScaler()\n",
    "Z_Train_Features = np.float16(Scale_Center.fit_transform(np.array(PCs_Train)))\n",
    "del PCs_Train \n",
    "gc.collect()\n",
    "\n",
    "Z_Test_Features = np.float16(Scale_Center.fit_transform(np.array(PCs_Test)))\n",
    "del PCs_Test \n",
    "gc.collect()\n",
    "\n",
    "print \"Scaling Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol) \n",
    "rfe = RFE(LogReg,Num_Features_To_Keep, step=RFE_Step)\n",
    "\n",
    "#reduce entire feature set to top x features for HandStart\n",
    "rfe.fit(Z_Train_Features, Train_Labels_HandStart.to_dense())\n",
    "HandStart_Train_Features = np.float16(rfe.transform(Z_Train_Features))\n",
    "gc.collect()\n",
    "\n",
    "HandStart_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"HandStart_Test Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "    \n",
    "    \n",
    "#reduce entire feature set to top x features for FirstDigitTouch   \n",
    "rfe.fit(Z_Train_Features, Train_Labels_FirstDigitTouch.to_dense())\n",
    "FirstDigitTouch_Train_Features = np.float16(rfe.transform(Z_Train_Features))   \n",
    "gc.collect()\n",
    "\n",
    "FirstDigitTouch_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"FirstDigitTouch Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "#reduce entire feature set to top x features for BothStartLoadPhase   \n",
    "rfe.fit(Z_Train_Features, Train_Labels_BothStartLoadPhase.to_dense())\n",
    "BothStartLoadPhase_Train_Features = np.float16(rfe.transform(Z_Train_Features))   \n",
    "gc.collect()\n",
    "\n",
    "BothStartLoadPhase_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"BothStartLoadPhase Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "#reduce entire feature set to top x features for LiftOff   \n",
    "rfe.fit(Z_Train_Features, Train_Labels_LiftOff.to_dense())\n",
    "LiftOff_Train_Features = np.float16(rfe.transform(Z_Train_Features))   \n",
    "gc.collect()\n",
    "\n",
    "LiftOff_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"LiftOff Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "#reduce entire feature set to top x features for Replace   \n",
    "rfe.fit(Z_Train_Features, Train_Labels_Replace.to_dense())\n",
    "Replace_Train_Features = np.float16(rfe.transform(Z_Train_Features))   \n",
    "gc.collect()\n",
    "\n",
    "Replace_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"Replace Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "#reduce entire feature set to top x features for BothReleased   \n",
    "rfe.fit(Z_Train_Features, Train_Labels_BothReleased.to_dense())\n",
    "BothReleased_Train_Features = np.float16(rfe.transform(Z_Train_Features))   \n",
    "gc.collect()\n",
    "\n",
    "BothReleased_Test_Features = np.float16(rfe.transform(Z_Test_Features))\n",
    "gc.collect()\n",
    "print \"BothReleased Features Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "\n",
    "\n",
    "#delete what isn't needed anymore to save memory. \n",
    "del Z_Train_Features\n",
    "del Z_Test_Features\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print BothReleased_Test_Features.shape[1], \"Features Remaining\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Different Models\n",
    "\n",
    "##Logistic Regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HandStart', ' Score =', 0.97995073846351799)\n",
      "('HandStart', 'AUC', 0.67537557227244893)\n",
      "('FirstDigitTouch', ' Score =', 0.98046541123273323)\n",
      "('FirstDigitTouch', 'AUC', 0.73081823859460271)\n",
      "('BothStartLoadPhase', ' Score =', 0.98046081594015089)\n",
      "('BothStartLoadPhase', 'AUC', 0.74570748025188682)\n",
      "('LiftOff', ' Score =', 0.97743251812842924)\n",
      "('LiftOff', 'AUC', 0.75471834199300536)\n",
      "('Replace', ' Score =', 0.97641695846774568)\n",
      "('Replace', 'AUC', 0.69387548993751025)\n",
      "('BothReleased', ' Score =', 0.97628829027544184)\n",
      "('BothReleased', 'AUC', 0.65981917217016595)\n",
      "102  Seconds to complete\n",
      "Overall Logistic Regression Score =  0.710052382537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l1'\n",
    "Convergence_tol = .01\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_LogReg(HandStart_Train_Features,Train_Labels_HandStart.to_dense(),HandStart_Test_Features,Test_Labels_HandStart.to_dense(), 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "AUC_FirstDigitTouch = predictCategory_LogReg(FirstDigitTouch_Train_Features,Train_Labels_FirstDigitTouch.to_dense(),FirstDigitTouch_Test_Features,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothStartLoadPhase = predictCategory_LogReg(BothStartLoadPhase_Train_Features,Train_Labels_BothStartLoadPhase.to_dense(),BothStartLoadPhase_Test_Features,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "AUC_LiftOff = predictCategory_LogReg(LiftOff_Train_Features,Train_Labels_LiftOff.to_dense(),LiftOff_Test_Features,Test_Labels_LiftOff.to_dense(), 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "AUC_Replace = predictCategory_LogReg(Replace_Train_Features,Train_Labels_Replace.to_dense(),Replace_Test_Features,Test_Labels_Replace.to_dense(), 'Replace', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothReleased = predictCategory_LogReg(BothReleased_Train_Features,Train_Labels_BothReleased.to_dense(),BothReleased_Test_Features,Test_Labels_BothReleased.to_dense(), 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "print int(time.time()-Start_Time), \" Seconds to complete\"\n",
    "print \"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HandStart', ' Score =', 0.96715744391445402)\n",
      "('HandStart', 'AUC', 0.84271222611360797)\n",
      "('FirstDigitTouch', ' Score =', 0.97057174630308707)\n",
      "('FirstDigitTouch', 'AUC', 0.81575167224714584)\n",
      "('BothStartLoadPhase', ' Score =', 0.96395452498460577)\n",
      "('BothStartLoadPhase', 'AUC', 0.82450103409558972)\n",
      "('LiftOff', ' Score =', 0.96218993263301078)\n",
      "('LiftOff', 'AUC', 0.8379948363275137)\n",
      "('Replace', ' Score =', 0.97826426608582173)\n",
      "('Replace', 'AUC', 0.80196850008054998)\n",
      "('BothReleased', ' Score =', 0.97734980286194817)\n",
      "('BothReleased', 'AUC', 0.7716553545378152)\n",
      "118  Seconds to complete\n",
      "Overall Logistic Regression Score =  0.815763937234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l2'\n",
    "Convergence_tol = .001\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_LogReg(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "AUC_FirstDigitTouch = predictCategory_LogReg(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothStartLoadPhase = predictCategory_LogReg(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "AUC_LiftOff = predictCategory_LogReg(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "AUC_Replace = predictCategory_LogReg(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothReleased = predictCategory_LogReg(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "print int(time.time()-Start_Time), \" Seconds to complete\"\n",
    "print \"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a failed attempt at parallel processing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Process\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l1'\n",
    "Convergence_tol = .01\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "Start_Time = time.time()\n",
    "def Log_Reg_HandStart():\n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "def Log_Reg_FirstDigitTouch():    \n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "def Log_Reg_BothStartLoadPhase():       \n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "def Log_Reg_LiftOff():       \n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "def Log_Reg_Replace():     \n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "def Log_Reg_BothReleased():    \n",
    "    predictCategory_LogReg(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "\n",
    "    \n",
    "\n",
    "def Overall_AUC(i):\n",
    "    if i == 1:\n",
    "        Log_Reg_HandStart()\n",
    "    if i == 2:\n",
    "        Log_Reg_FirstDigitTouch()\n",
    "    if i == 3:\n",
    "        Log_Reg_BothStartLoadPhase()\n",
    "    if i == 4:\n",
    "        Log_Reg_LiftOff()\n",
    "    if i == 5:\n",
    "        Log_Reg_Replace()\n",
    "    if i == 6:\n",
    "        Log_Reg_BothReleased()\n",
    "    \n",
    "# print Overall_AUC(AUC_Values)\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "pool = mp.Pool(processes=1)\n",
    "results = [pool.apply_async(Overall_AUC, args=(i,)) for i in range(1,7)]\n",
    "output = [p.get() for p in results]\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "print time.time()-Start_Time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015 11:30 A.M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set parameters for SVM\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "kernel = 'rbf'\n",
    "degree = 3\n",
    "\n",
    "#Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "def predictCategory_SVM(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "    SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "    SVC.fit(train_data, train_label)\n",
    "\n",
    "    prob = SVC.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "AUC_HandStart = predictCategory_SVM(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "AUC_FirstDigitTouch = predictCategory_SVM(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "AUC_BothStartLoadPhase = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "AUC_LiftOff = predictCategory_SVM(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "AUC_Replace = predictCategory_SVM(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "AUC_BothReleased = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "print \"Overall SVM Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set parameters for Neural Network model\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "n_components = 10\n",
    "learning_rate = .1\n",
    "batch_size = 20000\n",
    "n_iter =1\n",
    "\n",
    "\n",
    "#Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "def predictCategory_Neural_Network(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "    SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "    SVC.fit(train_data, train_label)\n",
    "\n",
    "    prob = SVC.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "AUC_HandStart = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "AUC_FirstDigitTouch = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "AUC_BothStartLoadPhase = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "AUC_LiftOff = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "AUC_Replace = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "AUC_BothReleased = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "print \"Overall Neural Network Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
