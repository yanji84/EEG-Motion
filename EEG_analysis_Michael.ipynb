{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "\n",
    "from pandas import *\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the test data into a single csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438144612.17 Merging...\n",
      "1438144612.17 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_data.csv\n",
      "1438144613.34 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_data.csv\n",
      "Merge Complete: 3.87000012398 total seconds\n",
      "1438144616.05 Merging...\n",
      "1438144616.05 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_events.csv\n",
      "1438144616.36 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_events.csv\n",
      "Merge Complete: 1.00999999046 total seconds\n",
      "1438144617.06 Merging...\n",
      "1438144617.06 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_data.csv\n",
      "Merge Complete: 1.37999987602 total seconds\n",
      "1438144618.44 Merging...\n",
      "1438144618.44 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_events.csv\n",
      "Merge Complete: 0.238000154495 total seconds\n"
     ]
    }
   ],
   "source": [
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_data_filenames = glob.glob(path + \"/subj1_series[1-2]*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "train_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_event_filenames = glob.glob(path + \"/subj1_series[1-2]*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "train_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same thing and now get your testing data\n",
    "train_data_filenames = glob.glob(path + \"/subj1_series3*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "test_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "test_event_filenames = glob.glob(path + \"/subj1_series3*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "test_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a simple model and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,0,0,0,0,0' '1,0,0,0,0,0' '0,1,0,0,0,0' '0,1,1,0,0,0' '0,0,1,0,0,0'\n",
      " '0,0,0,1,0,0' '0,0,0,0,1,0' '0,0,0,0,1,1' '0,0,0,0,0,1' '0,1,1,1,0,0'\n",
      " '0,0,1,1,0,0']\n"
     ]
    }
   ],
   "source": [
    "Train_Labels_HandStart =  train_events['HandStart']\n",
    "Train_Labels_FirstDigitTouch =  train_events['FirstDigitTouch']\n",
    "Train_Labels_BothStartLoadPhase =  train_events['BothStartLoadPhase']\n",
    "Train_Labels_LiftOff =  train_events['LiftOff']\n",
    "Train_Labels_Replace =  train_events['Replace']\n",
    "Train_Labels_BothReleased =  train_events['BothReleased']\n",
    "Train_Labels_Combined = train_events.HandStart.map(str) + ',' + train_events.FirstDigitTouch.map(str) + ',' + train_events.BothStartLoadPhase.map(str) + ',' + train_events.LiftOff.map(str) + ',' + train_events.Replace.map(str) + ',' + train_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "Train_Data = train_data.iloc[0:,1:] # select only column data\n",
    "\n",
    "\n",
    "Test_Labels_HandStart =  test_events['HandStart']\n",
    "Test_Labels_FirstDigitTouch =  test_events['FirstDigitTouch']\n",
    "Test_Labels_BothStartLoadPhase =  test_events['BothStartLoadPhase']\n",
    "Test_Labels_LiftOff =  test_events['LiftOff']\n",
    "Test_Labels_Replace =  test_events['Replace']\n",
    "Test_Labels_BothReleased =  test_events['BothReleased']\n",
    "Test_Labels_Combined = test_events.HandStart.map(str) + ',' + test_events.FirstDigitTouch.map(str) + ',' + test_events.BothStartLoadPhase.map(str) + ',' + test_events.LiftOff.map(str) + ',' + test_events.Replace.map(str) + ',' + test_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "Test_Data = test_data.iloc[0:,1:] # select only data columns\n",
    "\n",
    "print pd.unique(Train_Labels_Combined.values.ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "    df.drop(df.columns[[14,15,19,20,21,24,25,26,27,28,29,30,31]], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "#bin the time \n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extractFeatures_PCA(df,Test_df, PercentVarExplained):\n",
    "    data = df.as_matrix()\n",
    "    print len(data)\n",
    "    test_data = Test_df.as_matrix()\n",
    "    pca = PCA()\n",
    "    pca.fit(data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    PCA_Projections = pca.transform(data)[:,0:NumPCs]\n",
    "    PCA_Projections_Test = pca.transform(test_data)[:,0:NumPCs]\n",
    "    return PCA_Projections,PCA_Projections_Test\n",
    "\n",
    "\n",
    "\n",
    "#return rolling mean of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_mean(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_mean(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'mean' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling variance of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_var(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_var(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'var' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling min of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_min(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_min(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'min' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling max of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_max(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_max(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'max' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_skew(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_skew(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'skew' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_kurt(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_kurt(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'kurt' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'Pct'+ quantile + \"_\" + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_corr(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(pd.rolling_corr(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_cov(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(rolling_cov(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n",
      "   Fp1  Fp2   F7   F3   Fz   F4   F8  FC5  FC1  FC2  FC6   T7   C3   Cz  TP9  \\\n",
      "0  -31  363  211  121  211   15  717  279   35  158  543 -166  192  230  128   \n",
      "1  -29  342  216  123  222  200  595  329   43  166  495 -138  201  233  185   \n",
      "2 -172  278  105   93  222  511  471  280   12  177  534 -163  198  207  145   \n",
      "3 -272  263  -52   99  208  511  428  261   27  180  525 -310  212  221  115   \n",
      "4 -265  213  -67   99  155  380  476  353   32  165  507 -320  242  230  180   \n",
      "\n",
      "   CP5  CP1   P7   P3  \n",
      "0   59  272  536  348  \n",
      "1   47  269  529  327  \n",
      "2   52  250  511  319  \n",
      "3   41  276  521  336  \n",
      "4   89  288  550  324  \n"
     ]
    }
   ],
   "source": [
    "Train_Data = Remove_Channels(Train_Data)\n",
    "Test_Data = Remove_Channels(Test_Data)\n",
    "print \"Complete\"\n",
    "print Train_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391450\n"
     ]
    }
   ],
   "source": [
    "Pct_Variance_Explained = .7\n",
    "Rolling_Window = 50\n",
    "\n",
    "PCA_Projections_Train, PCA_Projections_Test = extractFeatures_PCA(Train_Data,Test_Data,Pct_Variance_Explained)\n",
    "\n",
    "PCA_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "PCA_Rolling_Var_Train = df_rolling_var(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "PCA_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "PCA_Rolling_Kurtosis = df_rolling_kurt(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "PCA_Rolling_Min_Train = df_rolling_min(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "PCA_Rolling_Max_Train = df_rolling_max(pd.DataFrame(PCA_Projections_Train),Rolling_Window)\n",
    "\n",
    "\n",
    "PCA_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "PCA_Rolling_Var_Test = df_rolling_var(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "PCA_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "PCA_Rolling_Kurtosis_Test = df_rolling_kurt(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "PCA_Rolling_Min_Test = df_rolling_min(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "PCA_Rolling_Max_Test = df_rolling_max(pd.DataFrame(PCA_Projections_Test),Rolling_Window)\n",
    "\n",
    "Feature_Array_Train = np.concatenate((PCA_Projections_Train,PCA_Rolling_Means_Train,PCA_Rolling_Var_Train,PCA_Rolling_Min_Train,PCA_Rolling_Max_Train),axis=1)\n",
    "Feature_Array_Test = np.concatenate((PCA_Projections_Test,PCA_Rolling_Means_Test,PCA_Rolling_Var_Test,PCA_Rolling_Min_Test,PCA_Rolling_Max_Test),axis=1)\n",
    "\n",
    "\n",
    "Train_Labels_Combined = np.array(Train_Labels_Combined)\n",
    "Test_Labels_Combined = np.array(Test_Labels_Combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rolling_Window = 20\n",
    "\n",
    "RawData_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "RawData_Rolling_Var_Train = df_rolling_var(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "RawData_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "# RawData_Rolling_Kurtosis = df_rolling_kurt(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "RawData_Rolling_Min_Train = df_rolling_min(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "RawData_Rolling_Max_Train = df_rolling_max(pd.DataFrame(Train_Data),Rolling_Window)\n",
    "\n",
    "\n",
    "RawData_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(Test_Data),Rolling_Window)\n",
    "RawData_Rolling_Var_Test = df_rolling_var(pd.DataFrame(Test_Data),Rolling_Window)\n",
    "RawData_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(Test_Data),Rolling_Window)\n",
    "# RawData_Rolling_Kurtosis_Test = df_rolling_kurt(pd.DataFrame(Test_Data),Rolling_Window)\n",
    "RawData_Rolling_Min_Test = df_rolling_min(pd.DataFrame(Test_Data),Rolling_Window)\n",
    "RawData_Rolling_Max_Test = df_rolling_max(pd.DataFrame(Test_Data),Rolling_Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RawData_Rolling_Min_Train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-08d0c9030e17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRawData_Feature_Array_Train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Skewness_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Min_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Max_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mRawData_Feature_Array_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTest_Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Skewness_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Min_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRawData_Rolling_Max_Test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RawData_Rolling_Min_Train' is not defined"
     ]
    }
   ],
   "source": [
    "RawData_Feature_Array_Train = np.concatenate((Train_Data,RawData_Rolling_Skewness_Train,RawData_Rolling_Min_Train,RawData_Rolling_Max_Train),axis=1)\n",
    "RawData_Feature_Array_Test = np.concatenate((Test_Data,RawData_Rolling_Skewness_Test,RawData_Rolling_Min_Test,RawData_Rolling_Max_Test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Logistic_Reg = LogisticRegression(C=.01,penalty = 'l2')  \n",
    "\n",
    "# Logistic_Reg.fit(Feature_Array_Train, Train_Labels_Combined)\n",
    "# print \"Combined Score =\", Logistic_Reg.score(Feature_Array_Test, Test_Labels_Combined)\n",
    "\n",
    "#try optimizing for C\n",
    "C_Values = []\n",
    "f1scores_Logistic_Reg = []\n",
    "highest_f1_Logistic_Reg = 0\n",
    "Best_C = 0\n",
    "for i in range(20,300,40):\n",
    "    c = np.true_divide(i,100) #range function doesn't accept float values. Do this as a workaround\n",
    "    Logistic_Reg = LogisticRegression(C = c, penalty = 'l2')       \n",
    "    Logistic_Reg.fit(Feature_Array_Train, Train_Labels_HandStart)\n",
    "    C_Values.append(c)\n",
    "    f1 = Logistic_Reg.score(Feature_Array_Test, Test_Labels_HandStart)\n",
    "    f1scores_Logistic_Reg.append(f1)\n",
    "    if f1 > highest_f1_Logistic_Reg:\n",
    "        highest_f1_Logistic_Reg = f1\n",
    "        Best_C = c\n",
    "\n",
    "\n",
    "print \"Logistic Regression Highest F1 =\", highest_f1_Logistic_Reg\n",
    "print \"Optimized C = \", Best_C\n",
    "plt.plot(C_Values,f1scores_Logistic_Reg)\n",
    "plt.show()\n",
    "\n",
    "Logistic_Reg.fit(Feature_Array_Train, Train_Labels_HandStart)\n",
    "print \"HandStart Score =\", Logistic_Reg.score(Feature_Array_Test, Test_Labels_HandStart)\n",
    "\n",
    "# Logistic_Reg.fit(df_Features_Train, Train_Labels_FirstDigitTouch)\n",
    "# print \"FirstDigitTouch Score =\", Logistic_Reg.score(df_Features_Test, Test_Labels_FirstDigitTouch)\n",
    "\n",
    "# Logistic_Reg.fit(df_Features_Train, Train_Labels_BothStartLoadPhase)\n",
    "# print \"BothStartLoadPhase Score =\", Logistic_Reg.score(df_Features_Test, Test_Labels_BothStartLoadPhase)\n",
    "\n",
    "# Logistic_Reg.fit(df_Features_Train, Train_Labels_LiftOff)\n",
    "# print \"LiftOff Score =\", Logistic_Reg.score(df_Features_Test, Test_Labels_LiftOff)\n",
    "\n",
    "# Logistic_Reg.fit(df_Features_Train, Train_Labels_Replace)\n",
    "# print \"Replace Score =\", Logistic_Reg.score(df_Features_Test, Test_Labels_Replace)\n",
    "\n",
    "# Logistic_Reg.fit(df_Features_Train, Train_Labels_BothReleased)\n",
    "# print \"BothReleased Score =\", Logistic_Reg.score(df_Features_Test, Test_Labels_BothReleased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart Score = 0.980685985277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -9.08241945e-04,   6.99887074e-05,  -3.52455316e-03,\n",
       "         -5.20895852e-04,   3.97556049e-04,  -6.43212664e-04,\n",
       "         -7.00928486e-05,  -1.43071587e-03,   8.86240452e-04,\n",
       "         -2.17075224e-03,   1.79851713e-04,  -1.58706337e-04,\n",
       "         -3.06558795e-03,   9.50746138e-04,  -2.52680511e-03,\n",
       "          3.72770433e-04,   2.46220752e-04,  -2.81254580e-03,\n",
       "          4.12971001e-04,  -7.39341416e-04,  -3.40829942e-04,\n",
       "          2.90830234e-04,   7.90061400e-04,  -1.14708669e-03,\n",
       "          8.87791889e-04,   1.31484697e-03,  -3.83253136e-04,\n",
       "         -1.88151252e-03,   1.41882009e-03,  -3.64354197e-03,\n",
       "          6.67256268e-04,   3.30141653e-03,  -2.58768003e-02,\n",
       "          6.49569088e-02,  -2.62481864e-02,  -4.97985572e-02,\n",
       "         -1.13873696e-03,  -6.86990858e-02,   8.28691921e-02,\n",
       "         -9.78026858e-02,  -2.88553646e-02,   4.23741738e-02,\n",
       "          3.49304940e-02,   6.62114710e-02,   1.24500284e-01,\n",
       "          9.78206331e-02,  -8.17871661e-02,  -9.12658280e-02,\n",
       "         -5.69469594e-02,   6.41228260e-02,  -9.36113072e-02,\n",
       "          2.79996156e-02,  -3.16556083e-02,  -6.57425814e-02,\n",
       "          5.52183809e-02,  -2.92345467e-04,   8.96408676e-02,\n",
       "         -8.69300141e-02,   1.66362974e-01,   4.75422580e-02,\n",
       "         -3.57363358e-02,   9.83875692e-02,  -1.91351397e-01,\n",
       "         -7.38693046e-02,   7.41639702e-04,  -2.36786414e-03,\n",
       "         -1.28584849e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_Reg = LogisticRegression(C = 1, penalty = 'l1',tol=.01)      \n",
    "Logistic_Reg.fit(RawData_Feature_Array_Train, Train_Labels_HandStart)\n",
    "print \"HandStart Score =\", Logistic_Reg.score(RawData_Feature_Array_Test, Test_Labels_HandStart)\n",
    "Logistic_Reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.94138575e-04,   2.17986863e-03,  -5.80203517e-03,\n",
       "         -2.12478661e-04,  -7.51109627e-05,   2.63227380e-04,\n",
       "          6.70923003e-04,  -1.35098331e-03,   3.08980826e-03,\n",
       "         -4.21881991e-03,   8.23844584e-04,   1.49916112e-03,\n",
       "         -1.68620524e-03,   1.98260935e-04,  -3.03162043e-03,\n",
       "         -9.56549728e-04,   2.45055473e-04,  -3.68700077e-03,\n",
       "          9.13275596e-04,   3.01850623e-04,  -4.13787212e-04,\n",
       "          3.08575558e-05,   2.07674908e-03,  -9.20516404e-04,\n",
       "          1.86429017e-03,   2.32790012e-03,  -1.46421479e-03,\n",
       "         -3.35731230e-03,   1.34172566e-03,  -4.89636966e-03,\n",
       "          2.44759584e-03,   9.48581760e-03,   1.04768621e-05,\n",
       "         -1.93478653e-05,   6.90656137e-06,  -1.56814275e-05,\n",
       "         -1.51406996e-04,   1.50997345e-05,  -1.59292434e-05,\n",
       "         -5.59498976e-06,   2.04969567e-04,  -1.23037789e-04,\n",
       "         -6.66066825e-06,   2.34130933e-07,  -2.31117120e-05,\n",
       "          4.07752634e-04,  -1.32834314e-04,  -2.83879230e-07,\n",
       "          2.01741299e-05,  -3.81187253e-05,  -8.11148711e-05,\n",
       "          2.42092721e-05,  -2.02129591e-05,   1.43699187e-05,\n",
       "          1.95987262e-06,  -1.87662751e-05,   1.84549384e-05,\n",
       "         -1.15828980e-04,   1.83747310e-05,  -1.23835367e-05,\n",
       "          4.47126055e-05,   3.70706400e-05,  -5.47990571e-05,\n",
       "          1.06405271e-05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2')\n",
    "model.fit(train_features, labels[class_to_test])\n",
    "\n",
    "# print auc\n",
    "predicted = [model.predict(test_feature)[0] for test_feature in test_features]\n",
    "predicted = np.array(predicted)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(t_labels[class_to_test], predicted, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart Score = 0.980663008814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.56300716e-04,   6.15128662e-04,  -2.99262791e-03,\n",
       "         -1.35495359e-04,   1.42485469e-03,  -4.38628395e-04,\n",
       "          8.20475761e-04,  -9.37546657e-04,   6.52229610e-04,\n",
       "         -1.28483640e-03,   1.02231844e-03,  -3.73184452e-06,\n",
       "         -3.05782606e-03,   1.77223605e-03,  -3.58648458e-03,\n",
       "         -2.71684348e-04,   1.71934429e-04,  -1.98686683e-03,\n",
       "          5.46144938e-04,  -7.60147392e-04,   4.83713221e-04,\n",
       "          2.05109558e-04,   8.91211210e-04,   6.72015859e-05,\n",
       "          1.30949706e-03,   1.68306197e-03,   2.39548849e-05,\n",
       "         -9.67449531e-04,   1.57336310e-03,  -3.51123277e-03,\n",
       "          1.24130092e-03,   4.08118280e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_Reg = LogisticRegression(C = 100, penalty = 'l1',tol=.01)      \n",
    "Logistic_Reg.fit(Train_Data, Train_Labels_HandStart)\n",
    "print \"HandStart Score =\", Logistic_Reg.score(Test_Data, Test_Labels_HandStart)\n",
    "Logistic_Reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
