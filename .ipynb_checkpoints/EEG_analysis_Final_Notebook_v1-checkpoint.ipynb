{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from pandas import *\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Change the path variables to run on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Main_Path = r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "Official_Test_Data_Path = r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set which subjects and series to use \n",
    "create a string to be put into a regular expression which extracts the data for the specified subjects and series numbers for training and testing. \n",
    "'[1-2]' =  1 and 2,   \n",
    "'[1,2]' = 1 and 2,    \n",
    "'[1-3]' = 1 THRU 3,    \n",
    "'[1,3]' = 1 AND 3,    \n",
    "'[4]' = 4, etc.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjects_to_use = '[1-2]'\n",
    "series_for_training = '[1-4]'\n",
    "series_for_testing = '[5]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and merge the specified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "    #df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "Overall_Start_Time = time.time() #start the overall timer \n",
    "\n",
    "\n",
    "Training_Data_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_training + \"*data.csv\"\n",
    "Training_Events_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_training + \"*events.csv\"\n",
    "\n",
    "Testing_Data_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_testing + \"*data.csv\"\n",
    "Testing_Events_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_testing + \"*events.csv\"\n",
    "\n",
    "\n",
    "train_data_filenames = glob.glob(Main_Path + Training_Data_RegEx_str)  #load the subjects and series defined above\n",
    "list_ = []\n",
    "Train_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Train_Array_Lengths.append(len(df))\n",
    "train_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "train_event_filenames = glob.glob(Main_Path + Training_Events_RegEx_str)  #load the subjects and series defined above\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "train_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same thing and now get your testing data\n",
    "test_data_filenames = glob.glob(Main_Path + Testing_Data_RegEx_str)  #load the subjects and series defined above\n",
    "list_ = []\n",
    "Test_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Test_Array_Lengths.append(len(df))\n",
    "test_data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "test_event_filenames = glob.glob(Main_Path + Testing_Events_RegEx_str) #load the subjects and series defined above\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "test_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the data into the 6 events and into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train_Labels_HandStart =  train_events['HandStart'].to_sparse(fill_value=0)\n",
    "Train_Labels_FirstDigitTouch =  train_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothStartLoadPhase =  train_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Train_Labels_LiftOff =  train_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Train_Labels_Replace =  train_events['Replace'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothReleased =  train_events['BothReleased'].to_sparse(fill_value=0)\n",
    "# Train_Labels_Combined = train_events.HandStart.map(str) + ',' + train_events.FirstDigitTouch.map(str) + ',' + train_events.BothStartLoadPhase.map(str) + ',' + train_events.LiftOff.map(str) + ',' + train_events.Replace.map(str) + ',' + train_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "Train_Data = train_data.iloc[0:,1:].astype('float32') # select only column data\n",
    "\n",
    "\n",
    "Test_Labels_HandStart =  test_events['HandStart'].to_sparse(fill_value=0)\n",
    "Test_Labels_FirstDigitTouch =  test_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothStartLoadPhase =  test_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Test_Labels_LiftOff =  test_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Test_Labels_Replace =  test_events['Replace'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothReleased =  test_events['BothReleased'].to_sparse(fill_value=0)\n",
    "\n",
    "\n",
    "Test_Data = test_data.iloc[0:,1:].astype('float32') # select only data columns\n",
    "\n",
    "#delete what isn't needed anymore to save memory. \n",
    "del train_data\n",
    "del test_data\n",
    "del train_events\n",
    "del test_events\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#bin the time \n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "##filters borrowed from Nihar. \n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extractFeatures_PCA(Train_Data,Test_Data, PercentVarExplained):\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    PCA_Projections = pca.transform(Train_Data)[:,0:NumPCs]\n",
    "    PCA_Projections_Test = pca.transform(Test_Data)[:,0:NumPCs]\n",
    "    return np.float32(PCA_Projections),np.float32(PCA_Projections_Test)\n",
    "\n",
    "\n",
    "\n",
    "#return rolling mean of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_mean(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_mean(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'mean' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling variance of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_var(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_var(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'var' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling min of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_min(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_min(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'min' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling max of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_max(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_max(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'max' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_skew(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_skew(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'skew' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_kurt(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_kurt(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'kurt' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'Pct'+ quantile + \"_\" + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_corr(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(pd.rolling_corr(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_cov(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(rolling_cov(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Feature Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes rolling statistics for different windows of time for each column of the dataframe and combines them into a single multi-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Windows = [10,100,500,700]\n",
    "\n",
    "Test_Features = []\n",
    "Train_Features = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "for w in Windows:\n",
    "    Raw_Data_Rolling_Window = w\n",
    "    \n",
    "    \n",
    "    RawData_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Train = df_rolling_min(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Train = df_rolling_max(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "\n",
    "    RawData_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Test = df_rolling_min(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Test = df_rolling_max(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "    print \"Calculations With Window of\", w, \"Complete: \", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "    \n",
    "    #Append all the features into 1\n",
    "    Train_Features.append(np.concatenate((RawData_Rolling_Means_Train,RawData_Rolling_Skewness_Train,RawData_Rolling_Min_Train,RawData_Rolling_Max_Train),axis = 1))\n",
    "    Test_Features.append(np.concatenate((RawData_Rolling_Means_Test,RawData_Rolling_Skewness_Test,RawData_Rolling_Min_Test,RawData_Rolling_Max_Test),axis = 1))\n",
    "    \n",
    "\n",
    "del RawData_Rolling_Means_Train\n",
    "del RawData_Rolling_Means_Test\n",
    "\n",
    "del RawData_Rolling_Skewness_Train\n",
    "del RawData_Rolling_Skewness_Test\n",
    "\n",
    "del RawData_Rolling_Min_Train\n",
    "del RawData_Rolling_Min_Test\n",
    "\n",
    "del RawData_Rolling_Max_Train\n",
    "del RawData_Rolling_Max_Test\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features),axis = 1))\n",
    "gc.collect() \n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features,Test_Data),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features,Train_Data),axis = 1))\n",
    "\n",
    "del Train_Data\n",
    "del Test_Data\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print Train_Features.shape[1], \"Total Resultant Features\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add additional features here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Nihar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Jeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Carson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of our feature set before modeling begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set the percent of the total variance explained we want explained by our subset of principal components. \n",
    "Pct_Variance_Explained = .93\n",
    "\n",
    "\n",
    "Start_Time = time.time() # begin timer\n",
    "\n",
    "# we must first scale the data. \n",
    "Scale_Center = StandardScaler()\n",
    "Z_Train_Features = np.float16(Scale_Center.fit_transform(np.array(Train_Features)))\n",
    "del Train_Features \n",
    "gc.collect()  #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "\n",
    "Z_Test_Features = np.float16(Scale_Center.fit_transform(np.array(Test_Features)))\n",
    "del Test_Features \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "PCs_Train,PCs_Test = extractFeatures_PCA(Z_Train_Features,Z_Test_Features,Pct_Variance_Explained)\n",
    "del Z_Train_Features,Z_Test_Features \n",
    "gc.collect()\n",
    "print \"PCA Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "print ''\n",
    "print PCs_Train.shape[1], \"Resultant Principal Components\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Different Models\n",
    "\n",
    "##Logistic Regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l2'\n",
    "Convergence_tol = .001\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_LogReg(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "AUC_FirstDigitTouch = predictCategory_LogReg(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothStartLoadPhase = predictCategory_LogReg(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "AUC_LiftOff = predictCategory_LogReg(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "AUC_Replace = predictCategory_LogReg(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothReleased = predictCategory_LogReg(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "print int(time.time()-Start_Time), \" Seconds to complete\"\n",
    "print \"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for SVM\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# C_Value = 1\n",
    "# kernel = 'rbf'\n",
    "# degree = 3\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_SVM(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_SVM(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_SVM(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_SVM(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_SVM(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall SVM Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for Neural Network model\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# n_components = 10\n",
    "# learning_rate = .1\n",
    "# batch_size = 20000\n",
    "# n_iter =1\n",
    "\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_Neural_Network(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall Neural Network Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print int(time.time()-Overall_Start_Time), \" Seconds to run all code\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
