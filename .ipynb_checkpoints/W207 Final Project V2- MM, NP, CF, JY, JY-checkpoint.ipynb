{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of Electroencephalography Measurements for Human Movements\n",
    "\n",
    "w207 Summer 2015, Final Project\n",
    "\n",
    "By Michael Marks, Nihar Patel, Carson Forter, Ji Yan, Jeff Yau\n",
    "\n",
    "![live brain](http://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/v2_article_large/public/2015/04/25/brain.jpg?itok=jIf_rHLW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Electroencephalography (EEG) is a non-invasive method of measuring brain activity. An array of sensors are arranged on a person's scalp. These sensors detect the electrical activity of the neurons firing immediately under the skull. Though this technology has limited spatial resolution and cannot detect electrical activity within deep brain structures, it has the advantage of having high temporal resolution; it measures electrical activity on the time scale that biological processes in the brain actually happen. This technology has many medical uses, one of which is allowing for a brain-to-computer interface. In particular, because the human brain controls movement in part with surface-oriented neurons, there is potential for using EEG to interpret muscle movements as signals from the brain. Advances in sensor hardware will allow for greater spatial resolution in the future, but the current challenge is in interpreting the raw brain signals into the intended muscle movement.\n",
    "\n",
    "On June 29th, 2015, Kaggle, a data science competition platform, introduced [a contest](https://www.kaggle.com/c/grasp-and-lift-eeg-detection) for EEG classification. The dataset was a series of EEG records from volunteers asked to repeatedly perform a task with their right hand. Each hand movement was recorded on video, and carefully documented into a matching dataset, in effect creating a set of raw sensor data matched with data of what arm movement corresponds to that data. Competitors were asked to use the sample dataset to produce the algorithm that most accurately predicts the correct arm movement from the raw data.\n",
    "\n",
    "In this workbook, we describe how our group approached the problem. We outline our preliminary research, our initial tests, how we handled the large dataset, and finally our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used\n",
    "We primarily use the [scikit-learn](http://scikit-learn.org/stable/index.html) python library for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The data can be downloaded from Kagle [here](https://www.kaggle.com/c/grasp-and-lift-eeg-detection/data). This workbook uses the folder structure data\\test\\ and data\\train. Each file contains data recorded at 500 Hz, so there are 500  rows of data for every second recorded. For each participant, there are seperate files for each session containing the recordings from 32 EEG sensors. Parallel to each of these files is a file describing which of 6 different arm motions are being performed at any given frame (1/500th of a second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Location of data can be changed here.\n",
    "#data_path = r'data/'\n",
    "data_path = r'/Users/JiAtUber/Desktop/EEG/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Here we define a function for loading a subset of data into a Pandas dataframe. This is necessary because the data is too large to hold in memory on an average desktop computer, and so we will need to call this function many times on different chunks of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def Replace_Subj_String_With_Dummy_Var(df):\n",
    "#     df_length = len(df)\n",
    "#     print df_length\n",
    "#     Dummy_df = pd.DataFrame(np.zeros(df_length, 12)) #create a dataframe of zeros for the twelve subjects\n",
    "#     Subject_String = df.iloc[0:1] # get the string of the subject. We just need the first one. \n",
    "#     Subject_int = int(re.findall(r'\\d+',str(Subject_String))[0]) #use regex to get the subject ID from the string.  \n",
    "#     Dummy_df.iloc[0:,Subject_int-1:Subject_int] = np.ones(df_length,1) # Set the subject's corresponding dummy variable = 1    \n",
    "#     return Dummy_df\n",
    "    \n",
    "def open_data(subjects_to_use=[1,2],series_for_training=[1-3]):\n",
    "    # Make a list of all the filenames with training data.\n",
    "    train_data_filenames = glob.glob(data_path + \"/subj\" + subjects_to_use +\n",
    "                                     \"_series\"+series_for_training + \"*data.csv\")\n",
    "\n",
    "    # Initialize an empty dataframe.\n",
    "    train_data= pd.DataFrame()\n",
    "    \n",
    "    # Load the dataframe with the contents of each file.\n",
    "    for file_ in train_data_filenames:\n",
    "        train_data = train_data.append(pd.read_csv(file_,index_col=None, header=0))\n",
    "        \n",
    "#     train_data.append(Replace_Subj_String_With_Dummy_Var(train_data.iloc[0:,:1])) #append the subject ID dummy variables\n",
    "    train_data = train_data.iloc[0:,1:] #delete the column of strings. They aren't needed anymore since we have the dummy variables. \n",
    "    \n",
    "    # Make a list of all the filenames with training labels.\n",
    "    train_labels_filenames = glob.glob(data_path + \"/subj\" + subjects_to_use +\n",
    "                                     \"_series\"+series_for_training + \"*events.csv\")\n",
    "    # Initialize an empty dataframe.\n",
    "    train_labels = pd.DataFrame()\n",
    "    \n",
    "    #Load the dataframe with the contents of each file.\n",
    "    for file_ in train_labels_filenames:\n",
    "        train_labels = train_labels.append(pd.read_csv(file_,index_col=None, header=0))\n",
    "        \n",
    "\n",
    "    # Return the resulting two dataframes.\n",
    "    return train_data, train_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing/Feature Engineering\n",
    "Our group spent the vast majority of our time researching the best way to pre-process the data. We attempted  feature engineering by transforming the raw data using both a priori knowledge and general data reduction techniques. We began by consulting with medical EEG researchers and reading literature on EEG processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Channels**\n",
    "\n",
    "Our medical EEG consultant did not have experience with interpreting muscle movement data, but was able to share some insights into neuroanatomy. The following brain map shows the different functional regions of the brain:\n",
    "\n",
    "![brain functional regions](http://lh3.ggpht.com/_RIjx_Mg4ZVM/TNbSn_XhNcI/AAAAAAAACeY/Abc73jPpSbU/image_thumb6.png)\n",
    "\n",
    "We anticipated that the frontal lobe of the brain would contain the relevant information for our problem. As the source of muscle movements, we belived that the region called the primary motor cortex is particularly important for predicting arm movements. The next image is a representation of what the primary motor cortex controls:\n",
    "\n",
    "![homunculus](http://brainconnection.brainhq.com/wp-content/uploads/2013/03/1b.gif)\n",
    "\n",
    "This is a coronal section of the brain (as if you were looking face-to-face with xray vision). The surface and about an inch under it, shaded in salmon color in the picture, is called the cortex. EEG sensors pick up the electrical firings of cells on the surface as shaded in blue, but note that the surface has grooves called gyri where the EEG sensors would have an especially hard time telling the difference between adjacently active sections of the cortex. The motor cortex, responsible for innervating our muscles, is almost always represented in the same spot and in the same order for humans, and we learned that individual differences likely wouldn't be picked up with the spatial resolution available using EEG. For example, if Sensor 1 is positioned above the motor strip at the position where the fingers are represented, we would expect that sensor to be active when the participants are closing and releasing their fingers from the object.\n",
    "\n",
    "Another important detail we learned was that the left side of the brain controls the right half of the body, so the cells we expect to be firing are on the left-side motor cortex.\n",
    "\n",
    "We produced a function to remove channels that we anticipated would contain more noise than information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "    df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Filtering**\n",
    "\n",
    "We identified a paper that highlighted the oscillatory nature of nerual network electrical activity. It suggested that for interpreting motor neuron activation, EEG sensor data in the 8-12 Hz and 16-24 Hz range should be used.\n",
    "\n",
    "We used the following bandpass filter function to eliminate frequencies in the raw sensor data outside of the desired ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(series, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, series)\n",
    "    return y\n",
    "\n",
    "def butterworth_filter(X,k,l):\n",
    "    '''\n",
    "      Butterworth Filter:\n",
    "      scipy.signal.butter(N, Wn, btype='low', analog=False, output='ba')[source]\n",
    "        N: the order of the filter, and 5 seems to be good enough\n",
    "        Wn: critical frequency, at which point the gain drops to 1/sqrt(2) that of \n",
    "            the passband (the \"-3dB point\")\n",
    "    '''\n",
    "    b,a = butter(5,k/250.0,btype='lowpass')\n",
    "    X = lfilter(b,a,X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_data(X):\n",
    "    scaler= StandardScaler()\n",
    "    # Just standardized the X\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    \n",
    "    # Define 2 x 20 features (based on a series of lowpass filters)\n",
    "    nFeaturesAdded=20\n",
    "    X_lowpass = np.zeros((np.shape(X_normalized)[0],nFeaturesAdded))\n",
    "    l=30\n",
    "    for i in range(nFeaturesAdded):\n",
    "        X_lowpass[:,i] = butterworth_filter(X[:,0],2-(i*0.1),l)\n",
    "        X_lowpass[:,i] = scaler.fit_transform(X_lowpass[:,i])\n",
    "    X_lowpass_squared = X_lowpass ** 2\n",
    "    X_preprocess = np.concatenate((X_lowpass, X_lowpass_squared),axis=1)\n",
    "    return X_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal Smoothing**\n",
    "\n",
    "The Kaggle dataset description explains that the labels can be off by +/- 75 ms. Also, we observed that the raw data has frequent dips and rises during both positive and negative conditions. Consequently, we decided that some method of compressing the signal temporarily and smoothing would be beneficial. This requires care because the competition rules state that future data cannot be used to predict earlier labels.\n",
    "\n",
    "We produced a number of functions designed to compress and smooth the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bin the time. This was an attempt to bin the entire dataset into features based on the amount of time since test start.\n",
    "# this proved unsuccessful\n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "# Return a rolling metric of each column in a pandas dataframe with a given window size. Returns df of same size. \n",
    "# Metric can be mean, var, min, max, skew, or kurt.\n",
    "def df_rolling_metric(df,window, metric,multiplier='none'):\n",
    "    # eval('pd.rolling'+metric+'(df.iloc[0:,i],'+ window +',min_periods = 0).fillna(0)')\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    if multiplier == 'abs':\n",
    "        df = df.abs()\n",
    "    if multiplier == 'square':\n",
    "        df = df**2\n",
    "    if multiplier == 'cube':\n",
    "        df = df**3   \n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = eval('pd.rolling_' + str(metric)+\"(df.iloc[0:,i],\"+str(window)+\",min_periods = 0).fillna(0)\")\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:metric + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "# Return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,quantile,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    return np.array(new_df.astype('float16'))\n",
    "\n",
    "\n",
    "# BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "# Return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_corr(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(pd.rolling_corr(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n",
    "\n",
    "# BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "# Return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_cov(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(rolling_cov(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Reduction**\n",
    "\n",
    "We were concerned about the size of the dataset. We could not fit all rows of data in memory, and this problem was compounded by our feature extraction methods that would produce extra columns in the data set. We decided to try some methods of feature reduction to alleviate the memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extract_PCs(Train_Data,Test_Data, PercentVarExplained):\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    return np.float32(pca.transform(Train_Data)[:,0:NumPCs]),np.float32(pca.transform(Test_Data)[:,0:NumPCs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Features\n",
    "After creating features and data reduction functions, we created a pipeline for testing these functions with a simple linear regression model. Through this process, we discovered that we would constantly observe around 98% accuracy; roughly 98% of labels were negative, and roughly only 2% of all labels in the dataset are positive. This is a reflection of quick motions that participants were asked to perform, and their relatively short duration compared to time at rest.\n",
    "\n",
    "We decided to judge the performance of our pre-processing steps using AUC scoring. **JEFF EXPLAIN HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time series nature of the data, the main features we went with were rolling statistics. These served as a means of smoothing the data, and extracting relevant statistics that could provide useful features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def regression_test(metric_string, train_data, train_label, test_data, test_label):\n",
    "    \n",
    "    Logistic_Reg = LogisticRegression(tol = .001)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(metric_string, \"AUC =\", round(roc_auc_score(test_label, prob[:,1]),4))\n",
    "\n",
    "def Test_Features():\n",
    "    #just test on subject 1, series 1,2 and 3\n",
    "    train_data, train_labels = open_data('[1]','[1,2]')\n",
    "    test_data, test_labels = open_data('[1]','[3]')\n",
    "\n",
    "\n",
    "\n",
    "    train_labels=train_labels['HandStart']\n",
    "    test_labels=test_labels['HandStart']\n",
    "    regression_test(\"lowpass filter\",preprocess_data(np.asarray(train_data)),train_labels,preprocess_data(np.asarray(test_data)),test_labels)\n",
    "\n",
    "    regression_test(\"Rolling Mean w/ Window of 100\",df_rolling_metric(train_data,100,\"mean\"),train_labels,df_rolling_metric(test_data,100,\"mean\"),test_labels )\n",
    "    regression_test(\"Rolling Mean w/ Window of 400\",df_rolling_metric(train_data,400,\"mean\"),train_labels,df_rolling_metric(test_data,400,\"mean\"),test_labels)\n",
    "    regression_test(\"Rolling Mean w/ Window of 700\",df_rolling_metric(train_data,700,\"mean\"),train_labels,df_rolling_metric(test_data,700,\"mean\"),test_labels)\n",
    "    regression_test(\"Rolling Mean w/ Window of 1000\",df_rolling_metric(train_data,1000,\"mean\"),train_labels,df_rolling_metric(test_data,1000,\"mean\"),test_labels)\n",
    "    regression_test(\"Rolling Mean w/ Window of 1500\",df_rolling_metric(train_data,1500,\"mean\"),train_labels,df_rolling_metric(test_data,1500,\"mean\"),test_labels)\n",
    "    regression_test(\"Rolling Mean w/ Window of 2000\",df_rolling_metric(train_data,2000,\"mean\"),train_labels,df_rolling_metric(test_data,2000,\"mean\"),test_labels)\n",
    "\n",
    "\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 100\",df_rolling_metric(train_data,100,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,100,\"mean\",\"abs\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 400\",df_rolling_metric(train_data,400,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,400,\"mean\",\"abs\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 700\",df_rolling_metric(train_data,700,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,700,\"mean\",\"abs\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 1000\",df_rolling_metric(train_data,1000,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,1000,\"mean\",\"abs\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 1500\",df_rolling_metric(train_data,1500,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,1500,\"mean\",\"abs\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Abs w/ Window of 2000\",df_rolling_metric(train_data,2000,\"mean\",\"abs\"),train_labels,df_rolling_metric(test_data,2000,\"mean\",\"abs\"),test_labels)\n",
    "\n",
    "    print(\"Let's try mean squared error as well to accentuate the large absolute values\")\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 100\",df_rolling_metric(train_data,100,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,100,\"mean\",\"square\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 400\",df_rolling_metric(train_data,400,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,400,\"mean\",\"square\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 700\",df_rolling_metric(train_data,700,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,700,\"mean\",\"square\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 1000\",df_rolling_metric(train_data,1000,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,1000,\"mean\",\"square\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 1500\",df_rolling_metric(train_data,1500,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,1500,\"mean\",\"square\"),test_labels)\n",
    "    regression_test(\"Rolling Mean_Square w/ Window of 2000\",df_rolling_metric(train_data,2000,\"mean\",\"square\"),train_labels,df_rolling_metric(test_data,2000,\"mean\",\"square\"),test_labels)\n",
    "\n",
    "\n",
    "    regression_test(\"Rolling Skew w/ Window of 100\",df_rolling_metric(train_data,100,\"skew\"),train_labels,df_rolling_metric(test_data,100,\"skew\"),test_labels)\n",
    "    regression_test(\"Rolling Skew w/ Window of 400\",df_rolling_metric(train_data,400,\"skew\"),train_labels,df_rolling_metric(test_data,400,\"skew\"),test_labels)\n",
    "    regression_test(\"Rolling Skew w/ Window of 700\",df_rolling_metric(train_data,700,\"skew\"),train_labels,df_rolling_metric(test_data,700,\"skew\"),test_labels)\n",
    "    regression_test(\"Rolling Skew w/ Window of 1000\",df_rolling_metric(train_data,1000,\"skew\"),train_labels,df_rolling_metric(test_data,1000,\"skew\"),test_labels)\n",
    "    regression_test(\"Rolling Skew w/ Window of 1500\",df_rolling_metric(train_data,1500,\"skew\"),train_labels,df_rolling_metric(test_data,1500,\"skew\"),test_labels)\n",
    "    regression_test(\"Rolling Skew w/ Window of 2000\",df_rolling_metric(train_data,2000,\"skew\"),train_labels,df_rolling_metric(test_data,2000,\"skew\"),test_labels)\n",
    "\n",
    "\n",
    "    regression_test(\"Rolling Min w/ Window of 100\",df_rolling_metric(train_data,100,\"min\"),train_labels,df_rolling_metric(test_data,100,\"min\"),test_labels)\n",
    "    regression_test(\"Rolling Min w/ Window of 400\",df_rolling_metric(train_data,400,\"min\"),train_labels,df_rolling_metric(test_data,400,\"min\"),test_labels)\n",
    "    regression_test(\"Rolling Min w/ Window of 700\",df_rolling_metric(train_data,700,\"min\"),train_labels,df_rolling_metric(test_data,700,\"min\"),test_labels)\n",
    "    regression_test(\"Rolling Min w/ Window of 1000\",df_rolling_metric(train_data,1000,\"min\"),train_labels,df_rolling_metric(test_data,1000,\"min\"),test_labels)\n",
    "    regression_test(\"Rolling Min w/ Window of 1500\",df_rolling_metric(train_data,1500,\"min\"),train_labels,df_rolling_metric(test_data,1500,\"min\"),test_labels)\n",
    "    regression_test(\"Rolling Min w/ Window of 2000\",df_rolling_metric(train_data,2000,\"min\"),train_labels,df_rolling_metric(test_data,2000,\"min\"),test_labels)\n",
    "\n",
    "\n",
    "    regression_test(\"Rolling Max w/ Window of 100\",df_rolling_metric(train_data,100,\"max\"),train_labels,df_rolling_metric(test_data,100,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 200\",df_rolling_metric(train_data,200,\"max\"),train_labels,df_rolling_metric(test_data,200,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 400\",df_rolling_metric(train_data,400,\"max\"),train_labels,df_rolling_metric(test_data,400,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 700\",df_rolling_metric(train_data,700,\"max\"),train_labels,df_rolling_metric(test_data,700,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 1000\",df_rolling_metric(train_data,1000,\"max\"),train_labels,df_rolling_metric(test_data,1000,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 1500\",df_rolling_metric(train_data,1500,\"max\"),train_labels,df_rolling_metric(test_data,1500,\"max\"),test_labels)\n",
    "    regression_test(\"Rolling Max w/ Window of 2000\",df_rolling_metric(train_data,2000,\"max\"),train_labels,df_rolling_metric(test_data,2000,\"max\"),test_labels)\n",
    "\n",
    "    # Lets try different percentiles as well. First we need to create a list of the percentiles we want to test. \n",
    "    Main_Pct_List = [.01,.05,.10,.25,.5,.75,.9,.95,.99]\n",
    "\n",
    "    for Pct in Main_Pct_List:\n",
    "        regression_test(\"Rolling Pct\"+str(Pct*100)+ \" w/ Window of 1000\",df_rolling_quantile(train_data,400,Pct),train_labels,df_rolling_quantile(test_data,400,Pct),test_labels)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# Test_Features()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Selection\n",
    "Based on the performance of our tested features, certain features were selected and combined. This is by no means the best way to do feature selection, but due to limited memory and overall time, this is the method we chose. Recursive feature extraction was attempted but eventually abandoned to to processing constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_Features(train_data,test_data):\n",
    "    Start_Time = time.time()   \n",
    "    Train_Features = np.asarray(train_data)\n",
    "    Test_Features = np.asarray(test_data)    \n",
    "\n",
    "    Train_Features = np.concatenate((Train_Features,preprocess_data(np.asarray(train_data))),axis=1) #apply lowpass filter\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,100,\"mean\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,400,\"mean\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1000,\"mean\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,700,\"skew\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1350,\"skew\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,2000,\"skew\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,100,\"mean\",'square')),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,400,\"mean\",'square')),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,700,\"mean\",'square')),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1000,\"mean\",'square')),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,100,\"min\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,700,\"min\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1000,\"min\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1500,\"min\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,100,\"max\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,700,\"max\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1000,\"max\")),axis=1)\n",
    "    Train_Features = np.concatenate((Train_Features,df_rolling_metric(train_data,1700,\"max\")),axis=1)\n",
    "    print(\"Train Features Complete:\",round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "    \n",
    "    Test_Features = np.concatenate((Test_Features,preprocess_data(np.asarray(test_data))),axis=1) #apply lowpass filter\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,100,\"mean\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,400,\"mean\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1000,\"mean\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,700,\"skew\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1350,\"skew\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,2000,\"skew\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,100,\"mean\",'square')),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,400,\"mean\",'square')),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,700,\"mean\",'square')),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1000,\"mean\",'square')),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,100,\"min\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,700,\"min\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1000,\"min\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1500,\"min\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,100,\"max\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,700,\"max\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1000,\"max\")),axis=1)\n",
    "    Test_Features = np.concatenate((Test_Features,df_rolling_metric(test_data,1500,\"max\")),axis=1)\n",
    "    print(\"All Features Complete:\",round((time.time()-Start_Time),2)/60, \" Minutes Total\")\n",
    "    print(Test_Features.shape[1],\"total features\")\n",
    "    return Train_Features, Test_Features\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dimensionality Reduction\n",
    "Because of the large dataset, we also used principal component analysis as a means of dimensionality reduction. Using a set of principal components that explained about 90% of the variation in the data seemed to strike a good balance between a reduced number of dimensions and information loss. The function we wrote for PCA allowed us to input a desired percent variance explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extract_PCs(Train_Features,Test_Features, PercentVarExplained):\n",
    "    Start_Time = time.time()   \n",
    "    Scale_Center = StandardScaler() #we must first scale and center the data.\n",
    "    Train_Features = np.float16(Scale_Center.fit_transform(np.array(Train_Features)))\n",
    "    gc.collect()  #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "    Test_Features = np.float16(Scale_Center.fit_transform(np.array(Test_Features)))\n",
    "    gc.collect()\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Features)\n",
    "    gc.collect()\n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    print('PCA Complete:',round((time.time()-Start_Time),2), \" seconds\")\n",
    "    print(NumPCs, 'Resultant Principal Components')\n",
    "\n",
    "    return np.float32(pca.transform(Train_Features)[:,0:NumPCs]),np.float32(pca.transform(Test_Features)[:,0:NumPCs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Model\n",
    "In order to test a model, we defined a couple of functions that would train a model and return an AUC value for each of the six label types:\n",
    " - Hand Start\n",
    " - First Digit Touch\n",
    " - Both Start Load Phase\n",
    " - Lift Off\n",
    " - Replace\n",
    " - Both Released\n",
    " \n",
    "This kaggle competition is judged on the mean of these six AUC values. Therefore, this was the metric we used to judge the performance of different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set parameters for Logistic Regression\n",
    "C_Value = 1\n",
    "penalty = 'l2'\n",
    "Convergence_tol = .001\n",
    "class_weight=\"auto\" \n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def Test_AUC(train_data, train_label, test_data, test_label,Category):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol,class_weight = class_weight)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    AUC = roc_auc_score(test_label, prob[:,1])\n",
    "    print(Category, \"AUC =\",round(AUC,4))\n",
    "    \n",
    "    return AUC\n",
    "\n",
    "def Test_Model(train_data,test_data,train_labels,test_labels):\n",
    "    print(\"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Testing The Model\n",
    "To test the model we simply looked at subject 1, series 1,2 and 3. Series 1 and 2 (~350,000 samples) were used for training and Series 3 (~200,000 samples) was used for testing. This allowed us to test different model parameters, and features without contraining memory. We looked at many different models, including:\n",
    " - Logistic Regression\n",
    " - Decision Trees\n",
    " - Random Forest\n",
    " - Boosted Decision Trees\n",
    " - KNN\n",
    " - Linear Discriminant Analysis\n",
    " - SVM\n",
    " - Random Forest Regressor\n",
    "#Was there anything else we tried?\n",
    "\n",
    "Ultimately, logistic regression resulted in the highest AUC during testing (~.93) and was typically the most computationally efficient. We feel like a neural network may have been a viable option; however, due to time constaints and issues with theano on windows, neural networks were not attempted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Features Complete:', 1.006, ' Minutes Elapsed')\n",
      "('All Features Complete:', 1.579, ' Minutes Total')\n",
      "(648, 'total features')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a409a5a6ba39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#perform dimensionality reduction. We'll use the PCs that explain 90% of the variance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_PCs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m.93\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5cc8a91d6f3e>\u001b[0m in \u001b[0;36mextract_PCs\u001b[0;34m(Train_Features, Test_Features, PercentVarExplained)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mStart_Time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mScale_Center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#we must first scale and center the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mTrain_Features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScale_Center\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Garbage collection (i.e. get rid of any outstanding unused memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mTest_Features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScale_Center\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_Features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \"\"\"\n\u001b[1;32m    347\u001b[0m         X = check_array(X, accept_sparse='csr', copy=self.copy,\n\u001b[0;32m--> 348\u001b[0;31m                         ensure_2d=False)\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwarn_if_not_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    350\u001b[0m                              array.ndim)\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 52\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "Start_Time = time.time()   \n",
    "\n",
    "#Open the data\n",
    "train_data, train_labels = open_data('[1]','[1,2]')\n",
    "test_data, test_labels = open_data('[1]','[3]')\n",
    "\n",
    "\n",
    "#transform into features    \n",
    "train_data, test_data = Create_Features(train_data,test_data)    \n",
    "\n",
    "\n",
    "#perform dimensionality reduction. We'll use the PCs that explain 90% of the variance.\n",
    "train_data, test_data = extract_PCs(train_data, test_data ,.93)\n",
    "\n",
    "\n",
    "Test_Model(train_data,test_data,train_labels,test_labels)\n",
    "\n",
    "print(round((time.time()-Start_Time),2)/60, \"Total Minutes to Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all data\n",
    "Through the feature selection processes we tried different methods to conserve memory, but no solution saved space  on the order of magnitude necessary to fit on a standard desktop computer (16 Gb RAM). We decided to use a model that permits training on batches of data sequentially. The options available in Scikit-Learn include:\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Perceptron\n",
    "- SGD Classifier\n",
    "- Passive Aggressive Classifier\n",
    "- SGD Regressor\n",
    "\n",
    "These functions all contain a .partial_fit() method for \"out of core\" (out of memory) learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier to use.\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(open_data('[1]','[1-3]',data_path+r'\\train'))\n",
    "print \"we did it\"\n",
    "\n",
    "# Minibatches of data to train on:\n",
    "subjects = ['[1,2]','[3,4]','[5,6]','[7,8]','[9,10]','[11,12]']\n",
    "series = ['[1,2]','[3,4]','[5,6]','[7,8]']\n",
    "\n",
    "# Cycle through the subjects and series\n",
    "\n",
    "for series_ in series:\n",
    "    for subject in subjects:\n",
    "        print \"Training on subjects:\", subject, \"series:\", series_\n",
    "        train_data, train_labels = open_data(subject, series_, data_path+r'\\train')\n",
    "        print train_data.shape\n",
    "        print train_data.ix[:,1:-1].shape\n",
    "        classifier.partial_fit(train_data.ix[:,1:-1], train_labels.ix[:,1:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predicted Output\n",
    "\n",
    "This is the helper method that generates an output csv file that contains the predicted probability of each event happening for each row of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = r'' # fill in the path to test directory\n",
    "\n",
    "def open_test_pd(subjects_to_use=[1,2],series_for_training=[1-3]):\n",
    "    test_data_filenames = glob.glob(test_path + \"/subj\" + subjects_to_use +\n",
    "                                     \"_series\"+series_for_training + \"*data.csv\")\n",
    "\n",
    "    # Initialize an empty dataframe.\n",
    "    test_data= pd.DataFrame()\n",
    "    \n",
    "    # Load the dataframe with the contents of each file.\n",
    "    for file_ in test_data_filenames:\n",
    "        test_data = test_data.append(pd.read_csv(file_,index_col=None, header=0))\n",
    "        \n",
    "    return test_data    \n",
    "\n",
    "def open_test_data(subjects_to_use=[1,2],series_for_training=[1-3]):\n",
    "    test_data = open_test_pd(subjects_to_use,series_for_training)\n",
    "    test_data = test_data.iloc[0:,1:] #delete the column of strings. They aren't needed anymore since we have the dummy variables. \n",
    "    return test_data\n",
    "\n",
    "outputFileName = 'submission.csv'\n",
    "predictedProb = []\n",
    "\n",
    "train_data, train_labels = open_data('[1]','[1]')\n",
    "test_data = open_test_data('[1]','[8,9]')\n",
    "test_pd = open_test_pd('[1]','[8,9]')\n",
    "\n",
    "#transform into features    \n",
    "train_data, test_data = Create_Features(train_data,test_data)    \n",
    "\n",
    "#perform dimensionality reduction. We'll use the PCs that explain 90% of the variance.\n",
    "train_data, test_data = extract_PCs(train_data, test_data ,.93)\n",
    "\n",
    "Train_Labels_HandStart =  train_labels['HandStart'].to_sparse(fill_value=0)\n",
    "Train_Labels_FirstDigitTouch =  train_labels['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothStartLoadPhase =  train_labels['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Train_Labels_LiftOff =  train_labels['LiftOff'].to_sparse(fill_value=0)\n",
    "Train_Labels_Replace =  train_labels['Replace'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothReleased =  train_labels['BothReleased'].to_sparse(fill_value=0)\n",
    "\n",
    "def SaveProb(train_label):    \n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol,class_weight=class_weight)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    predictedProb.append([Logistic_Reg.predict_proba(test_feature)[0][1] for test_feature in test_data])\n",
    "\n",
    "def GenerateOutput():\n",
    "    SaveProb(Train_Labels_HandStart.to_dense())\n",
    "    SaveProb(Train_Labels_FirstDigitTouch.to_dense())\n",
    "    SaveProb(Train_Labels_BothStartLoadPhase.to_dense())\n",
    "    SaveProb(Train_Labels_LiftOff.to_dense())\n",
    "    SaveProb(Train_Labels_Replace.to_dense())\n",
    "    SaveProb(Train_Labels_BothReleased.to_dense())\n",
    "\n",
    "    ids = np.array(test_pd[test_pd.columns[0]].values.astype(str))\n",
    "    cols = ['HandStart','FirstDigitTouch','BothStartLoadPhase','LiftOff','Replace','BothReleased']\n",
    "    submission = pd.DataFrame(index=ids, columns=cols, data=np.array(predictedProb).T)\n",
    "    submission.to_csv(outputFileName,index_label='id',float_format='%.3f') \n",
    "    \n",
    "Start_Time = time.time()\n",
    "GenerateOutput()  # it is an expensive operation to generate submission output for all subjects on all series\n",
    "print \"Generate submission output:\", int(time.time()-Start_Time), \" Seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not run model training utilizing the whole dataset for all subjects on one machine due to memory constraint. To get around it, instead, we run one model prediction for each subject separately and later combines the individual predicted output into one final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper Shell Scripts\n",
    "\n",
    "# This generates each output csv for each subject\n",
    "python msubmit.py 1 1.csv\n",
    "python msubmit.py 2 2.csv\n",
    "python msubmit.py 3 3.csv\n",
    "python msubmit.py 4 4.csv\n",
    "python msubmit.py 5 5.csv\n",
    "python msubmit.py 6 6.csv\n",
    "python msubmit.py 7 7.csv\n",
    "python msubmit.py 8 8.csv\n",
    "python msubmit.py 9 9.csv\n",
    "python msubmit.py 10 10.csv\n",
    "python msubmit.py 11 11.csv\n",
    "python msubmit.py 12 12.csv\n",
    "\n",
    "# This combines each subject output into one final submission csv\n",
    "cat 1.csv > final.csv\n",
    "tail -n +2 2.csv >> final.csv\n",
    "tail -n +2 3.csv >> final.csv\n",
    "tail -n +2 4.csv >> final.csv\n",
    "tail -n +2 5.csv >> final.csv\n",
    "tail -n +2 6.csv >> final.csv\n",
    "tail -n +2 7.csv >> final.csv\n",
    "tail -n +2 8.csv >> final.csv\n",
    "tail -n +2 9.csv >> final.csv\n",
    "tail -n +2 10.csv >> final.csv\n",
    "tail -n +2 11.csv >> final.csv\n",
    "tail -n +2 12.csv >> final.csv\n",
    "\n",
    "# Finally, we use a threshold to turn highest predicted probablity of each class into a positive case for final prediction\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "threshold = 0.2\n",
    "val = 1\n",
    "df = pd.read_csv('final.csv',index_col=None, header=0)\n",
    " \n",
    "df.HandStart[df.HandStart > threshold] = val\n",
    "df.FirstDigitTouch[df.FirstDigitTouch > threshold] = val\n",
    "df.BothStartLoadPhase[df.BothStartLoadPhase > threshold] = val\n",
    "df.LiftOff[df.LiftOff > threshold] = val\n",
    "df.Replace[df.Replace > threshold] = val\n",
    "df.BothReleased[df.BothReleased > threshold] = val\n",
    " \n",
    "df.to_csv('normalized.csv',index_label='id',float_format='%.3f') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
