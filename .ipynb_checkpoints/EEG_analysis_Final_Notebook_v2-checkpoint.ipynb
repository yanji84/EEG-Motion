{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from pandas import *\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "RandomForestClassifier\n",
    "import re\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from __future__ import print_function\n",
    "\n",
    "# For saving models to disk\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Change the path variables to run on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Main_Path = r'data\\train'\n",
    "Official_Test_Data_Path = r'data\\test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set which subjects and series to use \n",
    "create a string to be put into a regular expression which extracts the data for the specified subjects and series numbers for training and testing. \n",
    "'[1-2]' =  1 and 2,   \n",
    "'[1,2]' = 1 and 2,    \n",
    "'[1-3]' = 1 THRU 3,    \n",
    "'[1,3]' = 1 AND 3,    \n",
    "'[4]' = 4, etc.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjects_to_use = '[1]'\n",
    "series_for_training = '[1-2]'\n",
    "series_for_testing = '[3]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and merge the specified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439060620.64 Loading train data...\n",
      "1439060620.64 data\\train\\subj1_series1_data.csv\n",
      "1439060621.08 data\\train\\subj1_series2_data.csv\n",
      "Merge Complete: 1.40799999237 total seconds\n",
      "1439060622.06 Loading train labels...\n",
      "1439060622.06 data\\train\\subj1_series1_events.csv\n",
      "1439060622.16 data\\train\\subj1_series2_events.csv\n",
      "Merge Complete: 0.351000070572 total seconds\n",
      "1439060622.42 Loading dev data...\n",
      "1439060622.42 data\\train\\subj1_series3_data.csv\n",
      "Merge Complete: 0.787000179291 total seconds\n",
      "1439060623.21 Loading dev labels...\n",
      "1439060623.21 data\\train\\subj1_series3_events.csv\n",
      "Merge Complete: 0.186000108719 total seconds\n"
     ]
    }
   ],
   "source": [
    "#Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "    #df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "Overall_Start_Time = time.time() #start the overall timer \n",
    "\n",
    "\n",
    "Training_Data_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_training + \"*data.csv\"\n",
    "Training_Events_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_training + \"*events.csv\"\n",
    "\n",
    "Testing_Data_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_testing + \"*data.csv\"\n",
    "Testing_Events_RegEx_str = \"/subj\" + subjects_to_use + \"_series\"+series_for_testing + \"*events.csv\"\n",
    "\n",
    "\n",
    "train_data_filenames = glob.glob(Main_Path + Training_Data_RegEx_str)  #load the subjects and series defined above\n",
    "\n",
    "train_data= pd.DataFrame()\n",
    "Train_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print(time.time(), \"Loading train data...\")\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print(time.time(), file_)\n",
    "    train_data = train_data.append(Remove_Channels(pd.read_csv(file_,index_col=None, header=0)))\n",
    "\n",
    "print(\"Merge Complete:\", time.time()-Start_Time, \"total seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "train_event_filenames = glob.glob(Main_Path + Training_Events_RegEx_str)  #load the subjects and series defined above\n",
    "train_events = pd.DataFrame()\n",
    "\n",
    "Start_Time = time.time()\n",
    "print(time.time(), \"Loading train labels...\")\n",
    "\n",
    "for file_ in train_event_filenames:\n",
    "    print(time.time(), file_)\n",
    "    train_events = train_events.append(Remove_Channels(pd.read_csv(file_,index_col=None, header=0)))\n",
    "\n",
    "\n",
    "print(\"Merge Complete:\", time.time()-Start_Time, \"total seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same thing and now get your testing data\n",
    "test_data_filenames = glob.glob(Main_Path + Testing_Data_RegEx_str)  #load the subjects and series defined above\n",
    "test_data = pd.DataFrame()\n",
    "Start_Time = time.time()\n",
    "print(time.time(), \"Loading dev data...\")\n",
    "\n",
    "for file_ in test_data_filenames:\n",
    "    print(time.time(), file_)\n",
    "    Test_Array_Lengths.append(len(df))\n",
    "    test_data = test_data.append(Remove_Channels(pd.read_csv(file_,index_col=None, header=0)))\n",
    "\n",
    "print(\"Merge Complete:\", time.time()-Start_Time, \"total seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "test_event_filenames = glob.glob(Main_Path + Testing_Events_RegEx_str) #load the subjects and series defined above\n",
    "test_events = pd.DataFrame()\n",
    "Start_Time = time.time()\n",
    "print(time.time(), \"Loading dev labels...\")\n",
    "\n",
    "for file_ in test_event_filenames:\n",
    "    print(time.time(), file_)\n",
    "    test_events = test_events.append(Remove_Channels(pd.read_csv(file_,index_col=None, header=0)))\n",
    "\n",
    "print(\"Merge Complete:\", time.time()-Start_Time, \"total seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the data into the 6 events and into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Labels_HandStart =  train_events['HandStart'].to_sparse(fill_value=0)\n",
    "Train_Labels_FirstDigitTouch =  train_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothStartLoadPhase =  train_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Train_Labels_LiftOff =  train_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Train_Labels_Replace =  train_events['Replace'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothReleased =  train_events['BothReleased'].to_sparse(fill_value=0)\n",
    "# Train_Labels_Combined = train_events.HandStart.map(str) + ',' + train_events.FirstDigitTouch.map(str) + ',' + train_events.BothStartLoadPhase.map(str) + ',' + train_events.LiftOff.map(str) + ',' + train_events.Replace.map(str) + ',' + train_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "Train_Data = np.array(train_data.iloc[0:,1:]).astype('float32') # select only column data\n",
    "\n",
    "Train_ID_Strings = np.array(train_data.iloc[0:,:1]) # select the ID string to be used for features\n",
    "\n",
    "Test_Labels_HandStart =  test_events['HandStart'].to_sparse(fill_value=0)\n",
    "Test_Labels_FirstDigitTouch =  test_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothStartLoadPhase =  test_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Test_Labels_LiftOff =  test_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Test_Labels_Replace =  test_events['Replace'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothReleased =  test_events['BothReleased'].to_sparse(fill_value=0)\n",
    "\n",
    "\n",
    "Test_Data = test_data.iloc[0:,1:].astype('float32') # select only data columns\n",
    "\n",
    "Test_ID_Strings = np.array(test_data.iloc[0:,:1]) # select the ID string to be used for features\n",
    "\n",
    "\n",
    "#delete what isn't needed anymore to save memory. \n",
    "del train_data\n",
    "del test_data\n",
    "del train_events\n",
    "del test_events\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use RegEx to extract the subject number from the strings in column 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_Subject_IDs = [int(re.findall(r'\\d+',str(s))[0]) for s in Test_ID_Strings]\n",
    "Test_Subject_IDs = np.array(pd.get_dummies(Test_Subject_IDs))\n",
    "\n",
    "Train_Subject_IDs = [int(re.findall(r'\\d+',str(s))[0]) for s in Train_ID_Strings]\n",
    "Train_Subject_IDs = np.array(pd.get_dummies(Train_Subject_IDs))\n",
    "\n",
    "del Test_ID_Strings\n",
    "del Train_ID_Strings\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#bin the time \n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "##filters borrowed from Nihar. \n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extractFeatures_PCA(Train_Data,Test_Data, PercentVarExplained):\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    return np.float32(pca.transform(Train_Data)[:,0:NumPCs]),np.float32(pca.transform(Test_Data)[:,0:NumPCs])\n",
    "\n",
    "\n",
    "\n",
    "#return rolling mean of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_mean(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_mean(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'mean' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling variance of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_var(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_var(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'var' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling min of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_min(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_min(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'min' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#return rolling max of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_max(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_max(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'max' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_skew(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_skew(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'skew' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling kurtosis of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_kurt(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_kurt(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'kurt' + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'Pct'+ quantile + \"_\" + str(i)})\n",
    "    return np.array(new_df.astype('float32'))\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_corr(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(pd.rolling_corr(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n",
    "\n",
    "#BE CAREFUL NOT TO SUPPLY TOO MANY COLUMNS TO THIS FUNCTION. Returns 2^N columns, where N = intitial columns. \n",
    "#return rolling pairwise correlation of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_cov(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        list_.append(rolling_cov(df.iloc[0:,i],window,min_periods = 0))\n",
    "    return pd.concat(list_,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Feature Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes rolling statistics for different windows of time for each column of the dataframe and combines them into a single multi-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculations With Window of 10 Complete:  29  Seconds\n",
      "Calculations With Window of 100 Complete:  56  Seconds\n",
      "Calculations With Window of 500 Complete:  83  Seconds\n",
      "Calculations With Window of 700 Complete:  106  Seconds\n",
      "544 Total Resultant Features\n"
     ]
    }
   ],
   "source": [
    "Windows = [10,100,500,700]\n",
    "\n",
    "Test_Features = []\n",
    "Train_Features = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "for w in Windows:\n",
    "    Raw_Data_Rolling_Window = w\n",
    "    \n",
    "    \n",
    "    RawData_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Train = df_rolling_min(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Train = df_rolling_max(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "\n",
    "    RawData_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Test = df_rolling_min(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Test = df_rolling_max(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "    print(\"Calculations With Window of\", w, \"Complete: \", int(time.time()-Start_Time), \" Seconds\")\n",
    "\n",
    "    \n",
    "    #Append all the features into 1\n",
    "    Train_Features.append(np.concatenate((RawData_Rolling_Means_Train,RawData_Rolling_Skewness_Train,RawData_Rolling_Min_Train,RawData_Rolling_Max_Train),axis = 1))\n",
    "    Test_Features.append(np.concatenate((RawData_Rolling_Means_Test,RawData_Rolling_Skewness_Test,RawData_Rolling_Min_Test,RawData_Rolling_Max_Test),axis = 1))\n",
    "    \n",
    "\n",
    "del RawData_Rolling_Means_Train\n",
    "del RawData_Rolling_Means_Test\n",
    "\n",
    "del RawData_Rolling_Skewness_Train\n",
    "del RawData_Rolling_Skewness_Test\n",
    "\n",
    "del RawData_Rolling_Min_Train\n",
    "del RawData_Rolling_Min_Test\n",
    "\n",
    "del RawData_Rolling_Max_Train\n",
    "del RawData_Rolling_Max_Test\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features),axis = 1))\n",
    "gc.collect() \n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features,Test_Data),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features,Train_Data),axis = 1))\n",
    "\n",
    "del Train_Data\n",
    "del Test_Data\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(Train_Features.shape[1], \"Total Resultant Features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add additional features here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Nihar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Michael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add the subject ID dummy variables as features. \n",
    "\n",
    "Test_Features = np.float32(np.concatenate((Test_Features,Test_Subject_IDs),axis = 1))\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.float32( np.concatenate((Train_Features,Train_Subject_IDs),axis = 1))\n",
    "\n",
    "\n",
    "del Test_Subject_IDs\n",
    "del Train_Subject_IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Jeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Carson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of our feature set before modeling begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Complete: 55  Seconds\n",
      "\n",
      "91 Resultant Principal Components\n"
     ]
    }
   ],
   "source": [
    "#set the percent of the total variance explained we want explained by our subset of principal components. \n",
    "Pct_Variance_Explained = .93\n",
    "\n",
    "\n",
    "Start_Time = time.time() # begin timer\n",
    "\n",
    "# we must first scale the data. \n",
    "Scale_Center = StandardScaler()\n",
    "Z_Train_Features = np.float16(Scale_Center.fit_transform(np.array(Train_Features)))\n",
    "del Train_Features \n",
    "gc.collect()  #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "\n",
    "Z_Test_Features = np.float16(Scale_Center.fit_transform(np.array(Test_Features)))\n",
    "del Test_Features \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "PCs_Train,PCs_Test = extractFeatures_PCA(Z_Train_Features,Z_Test_Features,Pct_Variance_Explained)\n",
    "del Z_Train_Features,Z_Test_Features \n",
    "gc.collect()\n",
    "print(\"PCA Complete:\", int(time.time()-Start_Time), \" Seconds\")\n",
    "\n",
    "print('')\n",
    "print(PCs_Train.shape[1], \"Resultant Principal Components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Different Models\n",
    "\n",
    "##Logistic Regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart  Score = 0.95949709118\n",
      "HandStart AUC 0.915013668381\n",
      "FirstDigitTouch  Score = 0.966072954865\n",
      "FirstDigitTouch AUC 0.847717134414\n",
      "BothStartLoadPhase  Score = 0.966872535774\n",
      "BothStartLoadPhase AUC 0.851899993061\n",
      "LiftOff  Score = 0.969358589061\n",
      "LiftOff AUC 0.872607638987\n",
      "Replace  Score = 0.973434613582\n",
      "Replace AUC 0.893277665209\n",
      "BothReleased  Score = 0.971702188278\n",
      "BothReleased AUC 0.849871907252\n",
      "179  Seconds to complete\n",
      "Overall Logistic Regression Score =  0.87173133455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l2'\n",
    "Convergence_tol = .001\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    #print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_LogReg(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "AUC_FirstDigitTouch = predictCategory_LogReg(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothStartLoadPhase = predictCategory_LogReg(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "AUC_LiftOff = predictCategory_LogReg(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "AUC_Replace = predictCategory_LogReg(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothReleased = predictCategory_LogReg(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "print(int(time.time()-Start_Time), \" Seconds to complete\")\n",
    "print(\"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart AUC 0.720210208461\n",
      "5.02883333333  Minutes Elapsed\n"
     ]
    }
   ],
   "source": [
    "n_estimators=10\n",
    "n_jobs=7 #integer, optional (default=1) The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n",
    "\n",
    "criterion = \"mse\" # string, optional (default=”mse”)\n",
    "random_state = 0 # int, RandomState instance or None, optional (default=None)\n",
    "max_features = None # int, float, string or None, optional (default=None)\n",
    "max_depth = None # int or None, optional (default=None)\n",
    "min_samples_split = 2  # int, optional (default=2)\n",
    "min_samples_leaf = 1 # int, optional (default=1)\n",
    "max_leaf_nodes = None # int or None, optional (default=None)\n",
    "    \n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_RandomForestRegressor(train_data, train_label, test_data, test_label, Category,criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs):\n",
    "    RF_Regressor = RandomForestRegressor(criterion = criterion, random_state=random_state,max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes,n_estimators=n_estimators,n_jobs=n_jobs)    \n",
    "    RF_Regressor.fit(train_data, train_label)\n",
    "    #print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\n",
    "\n",
    "#     prob = RF_Regressor.score(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, RF_Regressor.predict(test_data)))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, RF_Regressor.predict(test_data))\n",
    "\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart' ,criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_FirstDigitTouch = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothStartLoadPhase = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_LiftOff = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_Replace = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothReleased = predictCategory_RandomForestRegressor(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes to complete\")\n",
    "# print(\"Overall Random Forest Regressor Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for SVM\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# C_Value = 1\n",
    "# kernel = 'rbf'\n",
    "# degree = 3\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_SVM(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_SVM(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_SVM(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_SVM(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_SVM(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall SVM Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for Neural Network model\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# n_components = 10\n",
    "# learning_rate = .1\n",
    "# batch_size = 20000\n",
    "# n_iter =1\n",
    "\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_Neural_Network(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall Neural Network Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart AUC 0.534551987485\n",
      "FirstDigitTouch AUC 0.522895227787\n",
      "BothStartLoadPhase AUC 0.512218852961\n",
      "LiftOff AUC 0.529106187303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fa49b5808a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mAUC_BothStartLoadPhase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_DecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_BothStartLoadPhase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_BothStartLoadPhase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BothStartLoadPhase'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mAUC_LiftOff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_DecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_LiftOff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_LiftOff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LiftOff'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mAUC_Replace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_DecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_Replace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_Replace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Replace'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mAUC_BothReleased\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_DecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_BothReleased\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_BothReleased\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BothReleased'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mStart_Time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Minutes to complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-fa49b5808a78>\u001b[0m in \u001b[0;36mpredictCategory_DecisionTree\u001b[1;34m(train_data, train_label, test_data, test_label, Category, criterion, splitter, random_state, max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredictCategory_DecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mDecisionTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mDecisionTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[0;32m    265\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = \"entropy\" # string, optional (default=”gini”)\n",
    "splitter = \"best\" # string, optional (default=”best”)\n",
    "random_state = 0 # int, RandomState instance or None, optional (default=None)\n",
    "max_features = None # int, float, string or None, optional (default=None)\n",
    "max_depth = None # int or None, optional (default=None)\n",
    "min_samples_split = 2  # int, optional (default=2)\n",
    "min_samples_leaf = 1 # int, optional (default=1)\n",
    "max_leaf_nodes = None # int or None, optional (default=None)\n",
    "    \n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_DecisionTree(train_data, train_label, test_data, test_label, Category,criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes):\n",
    "    DecisionTree = DecisionTreeClassifier(criterion = criterion, splitter = splitter, random_state=random_state,max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes)    \n",
    "    DecisionTree.fit(train_data, train_label)\n",
    "    #print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\n",
    "\n",
    "    prob = DecisionTree.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_DecisionTree(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart' ,criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_FirstDigitTouch = predictCategory_DecisionTree(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothStartLoadPhase = predictCategory_DecisionTree(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_LiftOff = predictCategory_DecisionTree(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_Replace = predictCategory_DecisionTree(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothReleased = predictCategory_DecisionTree(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes to complete\")\n",
    "# print(\"Overall Decision Tree Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart AUC 0.722234962383\n",
      "FirstDigitTouch AUC 0.661421928293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f400a0bc3e6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mAUC_HandStart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_HandStart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_HandStart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HandStart'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mAUC_FirstDigitTouch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_FirstDigitTouch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_FirstDigitTouch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FirstDigitTouch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mAUC_BothStartLoadPhase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_BothStartLoadPhase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_BothStartLoadPhase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BothStartLoadPhase'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mAUC_LiftOff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_LiftOff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_LiftOff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LiftOff'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mAUC_Replace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Labels_Replace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Labels_Replace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Replace'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-f400a0bc3e6b>\u001b[0m in \u001b[0;36mpredictCategory_RandomForest\u001b[1;34m(train_data, train_label, test_data, test_label, Category, criterion, random_state, max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes, n_estimators, n_jobs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredictCategory_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mRandomForest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mRandomForest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 279\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    658\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\multiprocessing\\pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\multiprocessing\\pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_estimators=20\n",
    "n_jobs=7 #integer, optional (default=1) The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n",
    "criterion = \"entropy\" # string, optional (default=”gini”) \"entropy\"\n",
    "random_state = 0 # int, RandomState instance or None, optional (default=None)\n",
    "max_features = None # int, float, string or None, optional (default=None)\n",
    "max_depth = None # int or None, optional (default=None)\n",
    "min_samples_split = 2  # int, optional (default=2)\n",
    "min_samples_leaf = 1 # int, optional (default=1)\n",
    "max_leaf_nodes = None # int or None, optional (default=None)\n",
    "    \n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_RandomForest(train_data, train_label, test_data, test_label, Category,criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs):\n",
    "    RandomForest = RandomForestClassifier(criterion = criterion, random_state=random_state,max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes,n_estimators=n_estimators,n_jobs=n_jobs)    \n",
    "    RandomForest.fit(train_data, train_label)\n",
    "    #print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\n",
    "\n",
    "    prob = RandomForest.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_RandomForest(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart' ,criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_FirstDigitTouch = predictCategory_RandomForest(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothStartLoadPhase = predictCategory_RandomForest(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_LiftOff = predictCategory_RandomForest(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_Replace = predictCategory_RandomForest(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothReleased = predictCategory_RandomForest(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased',criterion,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,n_jobs)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes to complete\")\n",
    "# print(\"Overall Random Forest Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Decision Tree\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandStart AUC 0.547386936725\n",
      "1.82383333333  Minutes Elapsed\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 10 # integer (default=50) The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "learning_rate = 10 #float, optional (default=1) Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "\n",
    "criterion = \"entropy\" # string, optional (default=”gini”)\n",
    "splitter = \"best\" # string, optional (default=”best”)\n",
    "random_state = 0 # int, RandomState instance or None, optional (default=None)\n",
    "max_features = None # int, float, string or None, optional (default=None)\n",
    "max_depth = None # int or None, optional (default=None)\n",
    "min_samples_split = 2  # int, optional (default=2)\n",
    "min_samples_leaf = 1 # int, optional (default=1)\n",
    "max_leaf_nodes = None # int or None, optional (default=None)\n",
    "    \n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_BoostedDecisionTree(train_data, train_label, test_data, test_label, Category,criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate):\n",
    "    DecisionTree = DecisionTreeClassifier(criterion = criterion, splitter = splitter, random_state=random_state,max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes)    \n",
    "    abc = AdaBoostClassifier(base_estimator=DecisionTree, n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "    \n",
    "    abc.fit(train_data, train_label)\n",
    "    #print(Category, \" Score =\", DecisionTree.score(test_data, test_label))\n",
    "\n",
    "    prob = abc.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart' ,criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_FirstDigitTouch = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothStartLoadPhase = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_LiftOff = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_Replace = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes Elapsed\")\n",
    "# AUC_BothReleased = predictCategory_BoostedDecisionTree(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased',criterion, splitter,random_state,max_features, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,n_estimators,learning_rate)\n",
    "# print(round((time.time()-Start_Time),2)/60, \" Minutes to complete\")\n",
    "# print(\"Overall Boosted Decision Tree Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(round((time.time()-Overall_Start_Time),2)/60, \" Minutes to run all code\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
