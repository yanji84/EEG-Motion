{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've included functions here to merge the test csv files into:\n",
    "\n",
    "- test_data.csv\n",
    "\n",
    "I've also included a script that separates the id field into subject, series, and frame number.\n",
    "\n",
    "Using pandas, these operations take minutes each, speed isn't an issue. However, memory is an issue. The test dataset is fine to use with all functions, but the training dataset is too large to merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for merging csv files in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The \"search_sample\" parameter accepts a template\n",
    "# string to look for in csvpath. For example, \n",
    "# *.csv would merge all .csv files found in\n",
    "# csvpath.\n",
    "\n",
    "def csv_merger(csvpath, search_sample, newcsv):\n",
    "    # Find all of the relevant files.\n",
    "    filenames = glob.glob(csvpath + search_sample)\n",
    "    \n",
    "    # Prepare an empty dataframe.\n",
    "    print datetime.datetime.now(), \"Merging...\"\n",
    "    new_dataframe = pd.DataFrame()\n",
    "\n",
    "    # Read each file into a pandas dataframe, and\n",
    "    # append it to the initial dataframe.\n",
    "    for file_ in filenames:\n",
    "        print datetime.datetime.now(), file_\n",
    "        new_dataframe = new_dataframe.append(pd.read_csv(file_,index_col=None, header=0))\n",
    "\n",
    "    # Save the results to a csv.\n",
    "    new_dataframe.to_csv(newcsv, index=False)\n",
    "\n",
    "    print datetime.datetime.now(), \"Merge Complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the test data into a single csv. Requires the following folder structure:\n",
    "  data\\ test\\ subj#_series$_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-25 09:48:15.784000 Merging...\n",
      "2015-07-25 09:48:15.790000 data\\test\\subj10_series10_data.csv\n",
      "2015-07-25 09:48:16.194000 data\\test\\subj10_series9_data.csv\n",
      "2015-07-25 09:48:16.609000 data\\test\\subj11_series10_data.csv\n",
      "2015-07-25 09:48:17.075000 data\\test\\subj11_series9_data.csv\n",
      "2015-07-25 09:48:17.547000 data\\test\\subj12_series10_data.csv\n",
      "2015-07-25 09:48:18.041000 data\\test\\subj12_series9_data.csv\n",
      "2015-07-25 09:48:18.566000 data\\test\\subj1_series10_data.csv\n",
      "2015-07-25 09:48:19.004000 data\\test\\subj1_series9_data.csv\n",
      "2015-07-25 09:48:19.487000 data\\test\\subj2_series10_data.csv\n",
      "2015-07-25 09:48:20.060000 data\\test\\subj2_series9_data.csv\n",
      "2015-07-25 09:48:20.625000 data\\test\\subj3_series10_data.csv\n",
      "2015-07-25 09:48:21.139000 data\\test\\subj3_series9_data.csv\n",
      "2015-07-25 09:48:21.682000 data\\test\\subj4_series10_data.csv\n",
      "2015-07-25 09:48:22.231000 data\\test\\subj4_series9_data.csv\n",
      "2015-07-25 09:48:22.771000 data\\test\\subj5_series10_data.csv\n",
      "2015-07-25 09:48:23.352000 data\\test\\subj5_series9_data.csv\n",
      "2015-07-25 09:48:23.951000 data\\test\\subj6_series10_data.csv\n",
      "2015-07-25 09:48:24.619000 data\\test\\subj6_series9_data.csv\n",
      "2015-07-25 09:48:25.279000 data\\test\\subj7_series10_data.csv\n",
      "2015-07-25 09:48:25.960000 data\\test\\subj7_series9_data.csv\n",
      "2015-07-25 09:48:26.655000 data\\test\\subj8_series10_data.csv\n",
      "2015-07-25 09:48:27.333000 data\\test\\subj8_series9_data.csv\n",
      "2015-07-25 09:48:28.006000 data\\test\\subj9_series10_data.csv\n",
      "2015-07-25 09:48:28.722000 data\\test\\subj9_series9_data.csv\n",
      "2015-07-25 09:48:50.946000 Merge Complete\n"
     ]
    }
   ],
   "source": [
    "path = r'data\\test'\n",
    "searchstring = \"/*data.csv\"\n",
    "newcsvname = 'test_data.csv'\n",
    "\n",
    "csv_merger(path, searchstring, newcsvname)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training data is too large to merge into a single file. Even when training, we will likely have to train iteratively, file by file. This code is provided as an example of how we'd merge them all together, if we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the training data only if you have at least 16 GB of memory.\n",
    "# I'm not sure if that's enough still.\n",
    "\n",
    "# path = r'data\\train'\n",
    "# searchstring = \"/*data.csv\"\n",
    "# newcsvname = 'train_data.csv'\n",
    "# csv_merger(path, searchstring, newcsvname)\n",
    "\n",
    "# print \"\\n-----------------\\n\"\n",
    "\n",
    "# Next, merge the training labels only if you have at least 16 GB of memory.\n",
    "# I'm not sure if that's enough still.\n",
    "\n",
    "# path = r'data\\train'\n",
    "# searchstring = \"/*events.csv\"\n",
    "# newcsvname = 'train_labels.csv'\n",
    "# csv_merger(path, searchstring, newcsvname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the split_id function to split the id field into separate fields for subject, session, and frame number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will read a csv or a dataframe,\n",
    "# and either return a dataframe or save a new csv\n",
    "# depending on which parameters you use. For example,\n",
    "# if you enter a csvpath but leave all other parameters\n",
    "# blank, it will read that csv and return a pandas\n",
    "# dataframe.\n",
    "\n",
    "def split_id(original_frame=pd.DataFrame(), csvpath='none', newcsv='none'):\n",
    "    \n",
    "    # If a csvpath is not provided, read the provided dataframe directly.\n",
    "    # Otherwise, read the csv into a pandas dataframe.\n",
    "    if csvpath == 'none':\n",
    "        originalframe = original_frame\n",
    "    else:\n",
    "        originalframe = pd.read_csv(csvpath, header=0)\n",
    "    print \"\\noriginal columns: \", originalframe.columns.values\n",
    "\n",
    "    # Create a new dataframe, separating out the subject, session, and frame number \n",
    "    # from the id column.\n",
    "    newframe = pd.DataFrame(originalframe.id.str.split('_').tolist())\n",
    "    print \"\\nnew column numbers: \", newframe.columns.values\n",
    "\n",
    "    # Rename the columns.\n",
    "    newframe = newframe.rename(columns={0: 'subject', 1: 'series', 2: 'frame'})\n",
    "    print \"\\nnew columns named: \", newframe.columns.values\n",
    "\n",
    "    # Merge the new frame with the original one.\n",
    "    newframe = pd.concat([newframe, originalframe], axis=1)\n",
    "    print \"\\nnew frame columns: \", newframe.columns.values\n",
    "    \n",
    "    # If a new csv file name wasn't specified, return the dataframe.\n",
    "    # Otherwise, save the results as a csv.\n",
    "    if newcsv=='none':\n",
    "        return newframe\n",
    "    else:\n",
    "        newframe.to_csv(newcsv, index=False)\n",
    "        print \"\\nComplete!\"\n",
    "\n",
    "# csvpath = 'data/train/subj1_series1_data.csv'\n",
    "# newcsv = 'new_results.csv'\n",
    "# split_id(csvpath=csvpath, newcsv=newcsv)\n",
    "# print split_id(csvpath=csvpath).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use subject 1 series 1 and subject 1 series 2 for training. Use subject 1 series 3 for development. Develop your own features. Use logistic regression as your model. We will then compare results and see which features work well.\n",
    "\n",
    "I'm experimenting with this part. -Nihar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fp1\n",
      "Processing Fp2\n",
      "Processing F7\n",
      "Processing F3\n",
      "Processing Fz\n",
      "Processing F4\n",
      "Processing F8\n",
      "Processing FC5\n",
      "Processing FC1\n",
      "Processing FC2\n",
      "Processing FC6\n",
      "Processing T7\n",
      "Processing C3\n",
      "Processing Cz\n",
      "Processing C4\n",
      "Processing T8\n",
      "Processing TP9\n",
      "Processing CP5\n",
      "Processing CP1\n",
      "Processing CP2\n",
      "Processing CP6\n",
      "Processing TP10\n",
      "Processing P7\n",
      "Processing P3\n",
      "Processing Pz\n",
      "Processing P4\n",
      "Processing P8\n",
      "Processing PO9\n",
      "Processing O1\n",
      "Processing Oz\n",
      "Processing O2\n",
      "Processing Fp1\n",
      "Processing Fp2\n",
      "Processing F7\n",
      "Processing F3\n",
      "Processing Fz\n",
      "Processing F4\n",
      "Processing F8\n",
      "Processing FC5\n",
      "Processing FC1\n",
      "Processing FC2\n",
      "Processing FC6\n",
      "Processing T7\n",
      "Processing C3\n",
      "Processing Cz\n",
      "Processing C4\n",
      "Processing T8\n",
      "Processing TP9\n",
      "Processing CP5\n",
      "Processing CP1\n",
      "Processing CP2\n",
      "Processing CP6\n",
      "Processing TP10\n",
      "Processing P7\n",
      "Processing P3\n",
      "Processing Pz\n",
      "Processing P4\n",
      "Processing P8\n",
      "Processing PO9\n",
      "Processing O1\n",
      "Processing Oz\n",
      "Processing O2\n",
      "                id       Fp1       Fp2        F7        F3        Fz  \\\n",
      "0  subj1_series1_0 -0.000000  0.000003  0.000002  0.000001  0.000002   \n",
      "1  subj1_series1_1 -0.000003  0.000036  0.000021  0.000012  0.000021   \n",
      "2  subj1_series1_2 -0.000018  0.000193  0.000113  0.000065  0.000115   \n",
      "3  subj1_series1_3 -0.000075  0.000698  0.000406  0.000237  0.000422   \n",
      "4  subj1_series1_4 -0.000257  0.001956  0.001117  0.000666  0.001202   \n",
      "\n",
      "         F4        F8       FC5       FC1    ...         TP10        P7  \\\n",
      "0  0.000000  0.000007  0.000003  0.000000    ...     0.000004  0.000005   \n",
      "1  0.000003  0.000070  0.000028  0.000004    ...     0.000038  0.000053   \n",
      "2  0.000029  0.000372  0.000155  0.000019    ...     0.000202  0.000287   \n",
      "3  0.000160  0.001336  0.000572  0.000070    ...     0.000732  0.001052   \n",
      "4  0.000615  0.003706  0.001634  0.000197    ...     0.002048  0.002981   \n",
      "\n",
      "         P3        Pz        P4        P8       PO9        O1        Oz  \\\n",
      "0  0.000003  0.000004  0.000001  0.000006  0.000003  0.000004  0.000002   \n",
      "1  0.000034  0.000038  0.000010  0.000061  0.000028  0.000045  0.000017   \n",
      "2  0.000185  0.000204  0.000054  0.000327  0.000152  0.000242  0.000090   \n",
      "3  0.000674  0.000747  0.000191  0.001199  0.000555  0.000882  0.000324   \n",
      "4  0.001906  0.002111  0.000529  0.003405  0.001579  0.002490  0.000906   \n",
      "\n",
      "         O2  \n",
      "0  0.000001  \n",
      "1  0.000012  \n",
      "2  0.000061  \n",
      "3  0.000215  \n",
      "4  0.000588  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "# Bandpass filter------------------------------------------\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Pre-process the training and development data.\n",
    "# Then save to CSV. ----------------------------------------\n",
    "\n",
    "# Load subject 1 series 1 and subject 1 series 2 for training.\n",
    "train_data_raw = pd.concat([pd.read_csv(\"data/train/subj1_series1_data.csv\", header=0),\n",
    "                       pd.read_csv(\"data/train/subj1_series2_data.csv\", header=0)])\n",
    "\n",
    "# Load subject 1 series 3 for development.\n",
    "dev_data_raw = pd.read_csv(\"data/train/subj1_series3_data.csv\", header=0)\n",
    "\n",
    "# Create a training dataframe.\n",
    "train_data=pd.DataFrame(train_data_raw['id'])\n",
    "\n",
    "# Process each column and add it to the training dataframe.\n",
    "for column in train_data_raw.columns.values[1:-1]:\n",
    "    print \"Processing\", column\n",
    "    train_data[column] = pd.Series(butter_bandpass_filter(train_data_raw[column], 8, 12, 500), index=train_data.index)\n",
    "\n",
    "\n",
    "# Create a development dataframe.\n",
    "dev_data=pd.DataFrame(dev_data_raw['id'])\n",
    "\n",
    "# Process each column and add it to the dev dataframe.\n",
    "for column in dev_data_raw.columns.values[1:-1]:\n",
    "    print \"Processing\", column\n",
    "    dev_data[column] = pd.Series(butter_bandpass_filter(dev_data_raw[column], 8, 12, 500), index=dev_data.index)\n",
    "\n",
    "\n",
    "#Check and then save the results.    \n",
    "print train_data.head()\n",
    "train_data.to_csv('train_data.csv',index=False)\n",
    "dev_data.to_csv('dev_data.csv',index=False)\n",
    "\n",
    "print \"complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'HandStart' 'FirstDigitTouch' 'BothStartLoadPhase' 'LiftOff'\n",
      " 'Replace' 'BothReleased']\n",
      "Training model for HandStart\n",
      " Coefficients: [[ 0.00039294 -0.00027013 -0.00052135  0.00143955 -0.00265022  0.001553\n",
      "   0.00047171 -0.00144589 -0.00052323 -0.00568038  0.0012132   0.00238694\n",
      "   0.00120376  0.00652494  0.0028678   0.00107466 -0.00196816  0.00045018\n",
      "   0.00021917 -0.00539265 -0.00249501 -0.00210144 -0.00035245  0.00069852\n",
      "  -0.00045426  0.00107202  0.00240749  0.00155399 -0.00229597  0.00235885\n",
      "  -0.0018295 ]]\n",
      "Training model for FirstDigitTouch\n",
      " Coefficients: [[ -6.80788281e-04   2.89367813e-04  -7.91349291e-04  -1.29861660e-03\n",
      "    2.02978102e-03   1.90340423e-03   3.84414983e-04  -1.63285819e-03\n",
      "    5.85165889e-03   6.23963246e-04  -1.49376257e-03   2.12158594e-03\n",
      "    1.07951551e-05   1.46881238e-03  -6.34171264e-04  -5.57913111e-04\n",
      "    2.56858502e-04   1.26008967e-03  -2.35707710e-03   4.54412238e-03\n",
      "   -1.02045342e-03  -3.03089510e-04   1.69975635e-04  -2.99317310e-03\n",
      "   -8.36607830e-04  -1.59401458e-03   2.58126427e-04  -2.52949060e-03\n",
      "    2.80792863e-03   1.26134993e-03  -2.94169809e-04]]\n",
      "Training model for BothStartLoadPhase\n",
      " Coefficients: [[  4.41204210e-04  -1.12623374e-04   9.80697739e-04  -6.46102152e-04\n",
      "   -4.79711038e-03   5.97865021e-04  -6.51787399e-06   2.47229865e-03\n",
      "   -3.75411218e-04   3.07867616e-03  -1.64589410e-03  -1.69922415e-03\n",
      "   -1.93816642e-03   1.93193669e-03   1.27949186e-03  -1.88716133e-03\n",
      "    1.15299266e-03   1.33067572e-04   2.73254945e-03  -7.08963660e-03\n",
      "    2.48552279e-04   1.73886506e-03  -1.59498000e-03   1.93687219e-05\n",
      "    8.71919103e-04   3.08356511e-03   3.38497664e-04   7.04010303e-04\n",
      "    2.74810889e-03  -5.85847420e-03   1.76761654e-03]]\n",
      "Training model for LiftOff\n",
      " Coefficients: [[ -1.25071312e-04  -2.33185367e-04   1.01588367e-04  -7.40206988e-05\n",
      "    1.06037681e-03  -5.99293423e-04  -2.13196162e-04  -1.11579161e-03\n",
      "    8.50499366e-03   2.82148098e-03  -7.41705218e-04   7.17351868e-05\n",
      "   -1.10809324e-03  -2.28363330e-03   5.39019864e-03   8.83762285e-04\n",
      "    2.86647504e-04  -1.03594085e-03  -4.02967380e-03  -3.67887965e-04\n",
      "   -4.33987734e-03  -4.07992436e-04   1.01424630e-03   1.48255118e-03\n",
      "    4.18704545e-03  -9.41556400e-04   1.54152952e-03  -2.32227917e-04\n",
      "    6.55310600e-04  -3.54774457e-03   1.48484265e-03]]\n",
      "Training model for Replace\n",
      " Coefficients: [[ 0.00122332 -0.00104279 -0.0008292   0.00050637 -0.00114433  0.00075215\n",
      "   0.00050168 -0.0008102   0.00042567 -0.00476593  0.00116667 -0.00148214\n",
      "   0.00133588  0.00258463 -0.00237102  0.00065714  0.00044051  0.00360185\n",
      "  -0.00524631  0.00616276 -0.00186647  0.00175296 -0.00320468 -0.00044118\n",
      "  -0.0012507   0.00054044  0.00020007 -0.00038117  0.00261644 -0.00011725\n",
      "  -0.00140308]]\n",
      "['id' 'HandStart' 'FirstDigitTouch' 'BothStartLoadPhase' 'LiftOff'\n",
      " 'Replace' 'BothReleased']\n",
      "(217614, 31)\n",
      "(217614, 7)\n",
      "\n",
      "Predictions--------\n",
      "\n",
      "\n",
      "HandStart\n",
      " Coefficients: [[ 0.00039294 -0.00027013 -0.00052135  0.00143955 -0.00265022  0.001553\n",
      "   0.00047171 -0.00144589 -0.00052323 -0.00568038  0.0012132   0.00238694\n",
      "   0.00120376  0.00652494  0.0028678   0.00107466 -0.00196816  0.00045018\n",
      "   0.00021917 -0.00539265 -0.00249501 -0.00210144 -0.00035245  0.00069852\n",
      "  -0.00045426  0.00107202  0.00240749  0.00155399 -0.00229597  0.00235885\n",
      "  -0.0018295 ]]\n",
      " Accuracy= 0.980699771154\n",
      "\n",
      "FirstDigitTouch\n",
      " Coefficients: [[ -6.80788281e-04   2.89367813e-04  -7.91349291e-04  -1.29861660e-03\n",
      "    2.02978102e-03   1.90340423e-03   3.84414983e-04  -1.63285819e-03\n",
      "    5.85165889e-03   6.23963246e-04  -1.49376257e-03   2.12158594e-03\n",
      "    1.07951551e-05   1.46881238e-03  -6.34171264e-04  -5.57913111e-04\n",
      "    2.56858502e-04   1.26008967e-03  -2.35707710e-03   4.54412238e-03\n",
      "   -1.02045342e-03  -3.03089510e-04   1.69975635e-04  -2.99317310e-03\n",
      "   -8.36607830e-04  -1.59401458e-03   2.58126427e-04  -2.52949060e-03\n",
      "    2.80792863e-03   1.26134993e-03  -2.94169809e-04]]\n",
      " Accuracy= 0.980699771154\n",
      "\n",
      "BothStartLoadPhase\n",
      " Coefficients: [[  4.41204210e-04  -1.12623374e-04   9.80697739e-04  -6.46102152e-04\n",
      "   -4.79711038e-03   5.97865021e-04  -6.51787399e-06   2.47229865e-03\n",
      "   -3.75411218e-04   3.07867616e-03  -1.64589410e-03  -1.69922415e-03\n",
      "   -1.93816642e-03   1.93193669e-03   1.27949186e-03  -1.88716133e-03\n",
      "    1.15299266e-03   1.33067572e-04   2.73254945e-03  -7.08963660e-03\n",
      "    2.48552279e-04   1.73886506e-03  -1.59498000e-03   1.93687219e-05\n",
      "    8.71919103e-04   3.08356511e-03   3.38497664e-04   7.04010303e-04\n",
      "    2.74810889e-03  -5.85847420e-03   1.76761654e-03]]\n",
      " Accuracy= 0.980699771154\n",
      "\n",
      "LiftOff\n",
      " Coefficients: [[ -1.25071312e-04  -2.33185367e-04   1.01588367e-04  -7.40206988e-05\n",
      "    1.06037681e-03  -5.99293423e-04  -2.13196162e-04  -1.11579161e-03\n",
      "    8.50499366e-03   2.82148098e-03  -7.41705218e-04   7.17351868e-05\n",
      "   -1.10809324e-03  -2.28363330e-03   5.39019864e-03   8.83762285e-04\n",
      "    2.86647504e-04  -1.03594085e-03  -4.02967380e-03  -3.67887965e-04\n",
      "   -4.33987734e-03  -4.07992436e-04   1.01424630e-03   1.48255118e-03\n",
      "    4.18704545e-03  -9.41556400e-04   1.54152952e-03  -2.32227917e-04\n",
      "    6.55310600e-04  -3.54774457e-03   1.48484265e-03]]\n",
      " Accuracy= 0.980699771154\n",
      "\n",
      "Replace\n",
      " Coefficients: [[ 0.00122332 -0.00104279 -0.0008292   0.00050637 -0.00114433  0.00075215\n",
      "   0.00050168 -0.0008102   0.00042567 -0.00476593  0.00116667 -0.00148214\n",
      "   0.00133588  0.00258463 -0.00237102  0.00065714  0.00044051  0.00360185\n",
      "  -0.00524631  0.00616276 -0.00186647  0.00175296 -0.00320468 -0.00044118\n",
      "  -0.0012507   0.00054044  0.00020007 -0.00038117  0.00261644 -0.00011725\n",
      "  -0.00140308]]\n",
      " Accuracy= 0.980699771154\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# Load subject 1 series 1 and subject 1 series 2 for training.\n",
    "train_data = pd.read_csv(\"train_data.csv\", index_col=0, header=0)\n",
    "\n",
    "train_labels = pd.concat([pd.read_csv(\"data/train/subj1_series1_events.csv\", header=0),\n",
    "                         pd.read_csv(\"data/train/subj1_series2_events.csv\", header=0)])\n",
    "print train_labels.columns.values\n",
    "\n",
    "# Note that I use index_col=0 to force the id line out of the dataframe (it becomes the index).\n",
    "# Otherwise, LogisticRegression() would try to use it as a variable and fail. The better\n",
    "# solution would be to subset it without the id field, but I had trouble with that and was\n",
    "# short on time. - Nihar\n",
    "\n",
    "\n",
    "# Train a seperate model for each category.\n",
    "categories = train_labels.columns.values[1:-1]\n",
    "models = []\n",
    "for category in categories:\n",
    "    print \"Training model for\", category\n",
    "    models.append(LogisticRegression(penalty='l2'))\n",
    "    models[-1].fit(train_data, train_labels[category])\n",
    "    print \" Coefficients:\", models[-1].coef_\n",
    "\n",
    "# Empty memory\n",
    "train_data = pd.DataFrame()\n",
    "train_labels = pd.DataFrame()\n",
    "    \n",
    "# Load the development set to check accuracy.\n",
    "dev_data = pd.read_csv(\"dev_data.csv\", index_col=0, header=0)\n",
    "dev_labels = pd.read_csv(\"data/train/subj1_series3_events.csv\", header=0)\n",
    "print dev_labels.columns.values\n",
    "\n",
    "print dev_data.shape\n",
    "print dev_labels.shape\n",
    "\n",
    "# Score the model with the development data.\n",
    "print \"\\nPredictions--------\\n\"\n",
    "categories = dev_labels.columns.values[1:-1]\n",
    "for model, category in zip(models, categories):\n",
    "    print \"\\n\", category\n",
    "    print \" Coefficients:\", model.coef_\n",
    "    print \" Accuracy=\", model.score(dev_data, dev_labels[category])\n",
    "\n",
    "print \"Complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training files...\n",
      "Training model...\n",
      "Loading test data...\n",
      "Making predictions...\n",
      "Complete! Script took 1292.69400001 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load a few random training data files into a pandas dataframe.\n",
    "path =r'data\\train'\n",
    "train_data_filenames, train_labels_filenames = glob.glob(path + \"/*data.csv\"), glob.glob(path + \"/*events.csv\")\n",
    "list1_ = []\n",
    "list2_ = []\n",
    "start = time.time()\n",
    "print \"Loading training files...\"\n",
    "\n",
    "\n",
    "for i in random.sample(range(len(train_data_filenames)), 2):\n",
    "    df1 = pd.read_csv(train_data_filenames[i], index_col=0, header=0)\n",
    "    df2 = pd.read_csv(train_labels_filenames[i], index_col=0, header=0)\n",
    "    list1_.append(df1)\n",
    "    list2_.append(df2)\n",
    "\n",
    "\n",
    "# pd.concat(list1_).to_csv('train_data.csv', index=False)\n",
    "# pd.concat(list2_).to_csv('train_labels.csv', index=False)\n",
    "# train_data = np.loadtxt(open('train_data.csv','rb'), delimiter=\",\",skiprows=1)\n",
    "# train_labels = np.loadtxt(open('train_labels.csv','rb'), delimiter = \",\", skiprows=1)\n",
    "\n",
    "train_data = pd.concat(list1_)[1:][range(32)].values.astype(float)\n",
    "train_labels = pd.concat(list2_)[1:][range(6)].values.astype(float)\n",
    "\n",
    "#train_data = pd.read_csv()\n",
    "#(list1_)[1:][range(32)].values.astype(float)\n",
    "#train_labels = pd.concat(list2_)[1:][range(6)].values.astype(float)\n",
    "\n",
    "print \"Training model...\"\n",
    "# Trying KNN model to start\n",
    "kborhood = KNeighborsClassifier(n_neighbors=5)\n",
    "kborhood.fit(train_data, train_labels)\n",
    "#gmnb = MultinomialNB().fit(train_data, train_labels)\n",
    "\n",
    "print \"Loading test data...\"\n",
    "# Load the test data\n",
    "# Will need to change index_col to 0 instead of 1 after I re-create the test data csv. \n",
    "test_data = pd.read_csv('test_data.csv', index_col=1, header=0)[1:][range(32)].values.astype(float)\n",
    "\n",
    "print \"Making predictions...\"\n",
    "# Make predictions\n",
    "predictions = pd.DataFrame(kborhood.predict(test_data[:]))\n",
    "\n",
    "# Save results\n",
    "predictions.to_csv('results.csv', index=False)\n",
    "print \"Complete! Script took\", (time.time() - start), \"seconds\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
